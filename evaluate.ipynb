{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from ultralytics.engine.results import Results\n",
    "\n",
    "from inference import Detector, Classificator, Inference\n",
    "from inference.Inference import quantify \n",
    "\n",
    "# object detection\n",
    "from utils.data import as_coco\n",
    "from utils.data.load import load_yolo, loader\n",
    "from utils.metrics.coco import CocoEvaluator\n",
    "from utils.metrics.utils import get_classify_ground_truth\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    # classification\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    average_precision_score,\n",
    "    # qunatification    \n",
    "    explained_variance_score,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    mean_absolute_percentage_error\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"val_source\": \"../datasets/orobanche_cummana/autosplit_val.txt\",\n",
    "    \"detect_model_path\": os.path.normpath(\"\"),\n",
    "    \"classify_model_path\": os.path.normpath(\"\"),\n",
    "    'results': 'results',\n",
    "    \n",
    "    \"classes\": {\n",
    "      0: \"background\",\n",
    "      1: \"healthy\",\n",
    "      2: \"necrotic\",  \n",
    "    },\n",
    "    #DETECTION\n",
    "    \"device\": None,\n",
    "    \"overlap\": 0.2,\n",
    "    \"patch_size\": 1024,\n",
    "    \"size_factor\": 1.0,\n",
    "    \"conf_thresh\": 0.25,\n",
    "    \"nms_iou_thresh\":  0.7,\n",
    "    \"max_patch_detections\": 300,\n",
    "    \"patch_per_batch\": 4,\n",
    "    \"pre_wbf_detections\":  3000,\n",
    "    \"wbf_ios_thresh\": 0.5,\n",
    "    \"max_detections\": 1000,\n",
    "    \n",
    "    #CLASSIFICATION (INFERENCE)\n",
    "    \"offset\": 0.2,\n",
    "    \n",
    "    #CLASSIFICATION (EVALUATION)\n",
    "    \"bg_iou_thresh\": 0.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_gt = load_yolo(config[\"val_source\"])\n",
    "coco_gt_detection, names_to_ids  = as_coco(detection_gt, config[\"classes\"])\n",
    "\n",
    "coco_detection_evaluator = CocoEvaluator(coco_gt_detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect = Detector(\n",
    "    model_path=config[\"detect_model_path\"],\n",
    "    device=config[\"device\"],\n",
    "    overlap=config[\"overlap\"],\n",
    "    patch_size=config[\"patch_size\"],\n",
    "    size_factor=config[\"size_factor\"],\n",
    "    conf_thresh=config[\"conf_thresh\"],\n",
    "    nms_iou_thresh=config[\"nms_iou_thresh\"],\n",
    "    max_patch_detections=config[\"max_patch_detections\"],\n",
    "    patch_per_batch=config[\"patch_per_batch\"],\n",
    "    pre_wbf_detections=config[\"pre_wbf_detections\"],\n",
    "    wbf_ios_thresh=config[\"wbf_ios_thresh\"],\n",
    "    max_detections=config[\"max_detections\"],\n",
    "    single_cls=True,\n",
    ")\n",
    "\n",
    "classify = Classificator(\n",
    "    model_path=config[\"detect_model_path\"], device=config[\"device\"]\n",
    ")\n",
    "\n",
    "inference = Inference(detect,classify,offset=config['offset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_predictions: dict[str, Results] = {}\n",
    "classify_predictions: dict[str, list]  = {}\n",
    "inference_predictions: dict[str, Results] = {}\n",
    "quantification_predictions = {}\n",
    "\n",
    "quantification_gt = {}\n",
    "classify_gt = {}\n",
    "\n",
    "for image_name, image in loader(config[\"val_source\"]):\n",
    "    detect_predictions[image_name] = inference.detect(image)\n",
    "    patches = inference.patch(image, detect_predictions[image_name].boxes)\n",
    "    #POTENTIAL BUG -> if no patches are detected, the classify_predictions will be empty\n",
    "    classify_predictions[image_name], confidences = inference.classify(patches)\n",
    "    classify_gt[image_name] = get_classify_ground_truth(\n",
    "        detect_predictions[image_name].boxes.xyxy, detection_gt[image_name][:4], config[\"bg_iou_thresh\"]\n",
    "    )\n",
    "    inference_predictions[image_name] = inference.merge_detect_and_classification(image,\n",
    "                                                                                detect_predictions[image_name],\n",
    "                                                                                classify_predictions[image_name],\n",
    "                                                                                confidences\n",
    "                                                                                )\n",
    "    quantification_gt[image_name] = quantify(detection_gt[image_name][:4],\n",
    "                                             detection_gt[image_name][4],\n",
    "                                             config[\"classes\"],)\n",
    "    quantification_predictions[image_name] = quantify(inference_predictions[image_name].boxes.xyxy,\n",
    "                                                      inference_predictions[image_name].boxes.cls,\n",
    "                                                      config[\"classes\"],)\n",
    "    \n",
    " \n",
    "classify_predictions_flat = []\n",
    "classify_gt_flat = []\n",
    "\n",
    "for image_name, predicts in classify_predictions.items():\n",
    "    assert len(predicts) == len(classify_gt[image_name]), f\"Image {image_name} has different number of predictions and gt\"\n",
    "    classify_predictions_flat.extend(predicts)\n",
    "    classify_gt_flat.extend(classify_gt[image_name])\n",
    "    \n",
    "class_num = len(config[\"classes\"])\n",
    "\n",
    "quantification_gt_flat = {\"areas\": np.zeros((len(quantification_gt), class_num), dtype=np.float64), \n",
    "                          \"counts\": np.zeros((len(quantification_gt), class_num), dtype=np.uint64)}\n",
    "quantification_predictions_flat = {\"areas\": np.zeros((len(quantification_predictions), class_num), dtype=np.float64), \n",
    "                                   \"counts\": np.zeros((len(quantification_predictions), class_num), dtype=np.uint64)}\n",
    "\n",
    "for idx, image_name in enumerate(quantification_gt.keys()):\n",
    "    gt = quantification_gt[image_name]\n",
    "    pred = quantification_predictions[image_name]\n",
    "    \n",
    "    quantification_gt_flat[\"areas\"][idx, :] = gt[\"areas\"]\n",
    "    quantification_gt_flat[\"counts\"][idx, :] = gt[\"counts\"]\n",
    "    \n",
    "    quantification_predictions_flat[\"areas\"][idx, :] = pred[\"areas\"]\n",
    "    quantification_predictions_flat[\"counts\"][idx, :] = pred[\"counts\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "detect_results = coco_detection_evaluator.eval(detect_predictions, names_to_ids, useCats=False)\n",
    "with open(os.path.join(config['results'], 'detect_coco'), \"w\") as f:\n",
    "    json.dump(config, f, indent=4)\n",
    "    \n",
    "\n",
    "# Separate AP and AR metrics for better visualization\n",
    "ap_metrics = {k: v for k, v in detect_results.items() if k.startswith(\"AP\")}\n",
    "ar_metrics = {k: v for k, v in detect_results.items() if k.startswith(\"AR\")}\n",
    "\n",
    "# Plot AP metrics\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=list(ap_metrics.keys()), y=list(ap_metrics.values()), palette=\"coolwarm\")\n",
    "plt.title(\"Average Precision (AP) Metrics\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot AR metrics\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=list(ar_metrics.keys()), y=list(ar_metrics.values()), palette=\"viridis\")\n",
    "plt.title(\"Average Recall (AR) Metrics\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classify metrics \n",
    "precision = precision_score(classify_gt_flat, classify_predictions_flat, average='macro')\n",
    "recall = recall_score(classify_gt_flat, classify_predictions_flat, average='macro')\n",
    "f1 = f1_score(classify_gt_flat, classify_predictions_flat, average='macro')\n",
    "accuracy = accuracy_score(classify_gt_flat, classify_predictions_flat)\n",
    "average_precision = average_precision_score(classify_gt_flat, classify_predictions_flat, average='macro')\n",
    "\n",
    "cm = confusion_matrix(classify_gt_flat, classify_predictions_flat, labels=config[\"classes\"].values())\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=config[\"classes\"].values(),\n",
    "            yticklabels=config[\"classes\"].values())\n",
    "plt.title(f'Confusion Matrix for classsification')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Graph classify metrics: \n",
    "metrics = {\n",
    "    \"Precision\": precision,\n",
    "    \"Recall\": recall,\n",
    "    \"F1 Score\": f1,\n",
    "    \"Accuracy\": accuracy,\n",
    "    \"Average Precision\": average_precision\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=list(metrics.keys()), y=list(metrics.values()), palette=\"viridis\")\n",
    "plt.title(\"Classification Metrics\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infere_results = coco_detection_evaluator.eval(detect_predictions, names_to_ids, useCats=False)\n",
    "with open(os.path.join(config['results'], 'detect_coco'), \"w\") as f:\n",
    "    json.dump(config, f, indent=4)\n",
    "    \n",
    "# Separate AP and AR metrics for better visualization\n",
    "ap_metrics = {k: v for k, v in infere_results.items() if k.startswith(\"AP\")}\n",
    "ar_metrics = {k: v for k, v in infere_results.items() if k.startswith(\"AR\")}\n",
    "\n",
    "# Plot AP metrics\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=list(ap_metrics.keys()), y=list(ap_metrics.values()), palette=\"coolwarm\")\n",
    "plt.title(\"Average Precision (AP) Metrics\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot AR metrics\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=list(ar_metrics.keys()), y=list(ar_metrics.values()), palette=\"viridis\")\n",
    "plt.title(\"Average Recall (AR) Metrics\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_metrics = {}\n",
    "for class_id, class_name in config[\"classes\"].items():\n",
    "    if class_id != 0:\n",
    "        gt_areas = quantification_gt_flat[\"areas\"][:, class_id]\n",
    "        pred_areas = quantification_predictions_flat[\"areas\"][:, class_id]\n",
    "        area_metrics[class_name] = {\n",
    "            \"Explained Variance\": explained_variance_score(gt_areas, pred_areas),\n",
    "            \"MAE\": mean_absolute_error(gt_areas, pred_areas),\n",
    "            \"R2\": r2_score(gt_areas, pred_areas),\n",
    "            \"MAPE\": mean_absolute_percentage_error(gt_areas, pred_areas),\n",
    "        }\n",
    "\n",
    "all_gt_areas = quantification_gt_flat[\"areas\"][:, 1:].sum(axis=1)\n",
    "all_pred_areas = quantification_predictions_flat[\"areas\"][:, 1:].sum(axis=1)\n",
    "area_metrics[\"All Classes\"] = {\n",
    "    \"Explained Variance\": explained_variance_score(all_gt_areas, all_pred_areas),\n",
    "    \"MAE\": mean_absolute_error(all_gt_areas, all_pred_areas),\n",
    "    \"R2\": r2_score(all_gt_areas, all_pred_areas),\n",
    "    \"MAPE\": mean_absolute_percentage_error(all_gt_areas, all_pred_areas),\n",
    "}\n",
    "\n",
    "for name, metrics in area_metrics.items():\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(x=metrics.keys(), y=metrics.values(), palette=\"coolwarm\")\n",
    "    plt.title(f\"Area Metrics for {name}\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_metrics = {}\n",
    "for class_id, class_name in config[\"classes\"].items():\n",
    "    if class_id != 0:\n",
    "        gt_counts = quantification_gt_flat[\"counts\"][:, class_id]\n",
    "        pred_counts = quantification_predictions_flat[\"counts\"][:, class_id]\n",
    "        count_metrics[class_name] = {\n",
    "            \"Explained Variance\": explained_variance_score(gt_counts, pred_counts),\n",
    "            \"MAE\": mean_absolute_error(gt_counts, pred_counts),\n",
    "            \"R2\": r2_score(gt_counts, pred_counts),\n",
    "            \"MAPE\": mean_absolute_percentage_error(gt_counts, pred_counts),\n",
    "        }\n",
    "\n",
    "all_gt_counts = quantification_gt_flat[\"counts\"][:, 1:].sum(axis=1)\n",
    "all_pred_counts = quantification_predictions_flat[\"counts\"][:, 1:].sum(axis=1)\n",
    "count_metrics[\"All Classes\"] = {\n",
    "    \"Explained Variance\": explained_variance_score(all_gt_counts, all_pred_counts),\n",
    "    \"MAE\": mean_absolute_error(all_gt_counts, all_pred_counts),\n",
    "    \"R2\": r2_score(all_gt_counts, all_pred_counts),\n",
    "    \"MAPE\": mean_absolute_percentage_error(all_gt_counts, all_pred_counts),\n",
    "}\n",
    "\n",
    "for name, metrics in count_metrics.items():\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(x=metrics.keys(), y=metrics.values(), palette=\"coolwarm\")\n",
    "    plt.title(f\"Count Metrics for {name}\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
