{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae749deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from inference import Detector, Classificator, Inference\n",
    "from inference.Inference import quantify \n",
    "\n",
    "# Object detection\n",
    "from data.load import load_yolo, loader, load_as_coco\n",
    "from metrics.coco import CocoEvaluator\n",
    "from metrics.utils import get_classify_ground_truth\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    explained_variance_score,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    mean_absolute_percentage_error\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c603460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"val_source\": \"../datasets/orobanche_cummana/autosplit_val.txt\",\n",
    "    \"detect_model_path\": os.path.normpath(\"./runs/detect/train2/weights/best.pt\"),\n",
    "    \"classify_model_path\": os.path.normpath(\"./runs/classify/train/weights/best.pt\"),\n",
    "    'results': 'results',\n",
    "    \n",
    "    \"to_classes\": {\n",
    "      0: \"background\",\n",
    "      1: \"healthy\",\n",
    "      2: \"necrotic\", \n",
    "    },\n",
    "    \n",
    "    #DETECTION\n",
    "    \"device\": 0,\n",
    "    \"overlap\": 0.2,\n",
    "    \"patch_size\": 1024,\n",
    "    \"size_factor\": 1.0,\n",
    "    \"conf_thresh\": 0.25,\n",
    "    \"nms_iou_thresh\":  0.7,\n",
    "    \"max_patch_detections\": 300,\n",
    "    \"patch_per_batch\": 4,\n",
    "    \"pre_wbf_detections\":  500,\n",
    "    \"wbf_ios_thresh\": 0.5,\n",
    "    \"max_detections\": 1000,\n",
    "    \n",
    "    #CLASSIFICATION (INFERENCE)\n",
    "    \"offset\": 0.1,\n",
    "    \"classify_img_size\": 224,\n",
    "    \n",
    "    #CLASSIFICATION (EVALUATION)\n",
    "    \"bg_iou_thresh\": 0.7,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7643de4a",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Initialize models"
   },
   "outputs": [],
   "source": [
    "detect = Detector(\n",
    "    model_path=config[\"detect_model_path\"],\n",
    "    device=config[\"device\"],\n",
    "    overlap=config[\"overlap\"],\n",
    "    patch_size=config[\"patch_size\"],\n",
    "    size_factor=config[\"size_factor\"],\n",
    "    conf_thresh=config[\"conf_thresh\"],\n",
    "    nms_iou_thresh=config[\"nms_iou_thresh\"],\n",
    "    max_patch_detections=config[\"max_patch_detections\"],\n",
    "    patch_per_batch=config[\"patch_per_batch\"],\n",
    "    pre_wbf_detections=config[\"pre_wbf_detections\"],\n",
    "    wbf_ios_thresh=config[\"wbf_ios_thresh\"],\n",
    "    max_detections=config[\"max_detections\"],\n",
    "    single_cls=True,\n",
    ")\n",
    "\n",
    "classify = Classificator(\n",
    "    model_path=config[\"classify_model_path\"], \n",
    "    device=config[\"device\"], \n",
    "    img_size=config[\"classify_img_size\"],\n",
    ")\n",
    "\n",
    "inference = Inference(detect, classify, offset=config['offset'])\n",
    "os.makedirs(config['results'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fb7e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_gt = {k: np.asarray(v) for k, v in load_yolo(config[\"val_source\"]).items()}\n",
    "coco_gt_detection, names_to_ids = load_as_coco(config[\"val_source\"], config[\"to_classes\"])\n",
    "coco_detection_evaluator = CocoEvaluator(coco_gt_detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a375bad0",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Process predictions and ground truth"
   },
   "outputs": [],
   "source": [
    "\n",
    "def process_image(image_name, image, inference: Inference, detection_gt, config):\n",
    "    \"\"\"Process a single image through the detection and classification pipeline.\"\"\"\n",
    "\n",
    "\n",
    "    class_num = len(config[\"to_classes\"])\n",
    "    results = {\n",
    "        \"image_name\": image_name,\n",
    "        \"detect_prediction\": None,\n",
    "        \"classify_prediction\": [],\n",
    "        \"classify_gt\": [],\n",
    "        \"inference_prediction\": {},\n",
    "        \"quantification_gt\": {\n",
    "            \"areas\": np.zeros(class_num, dtype=np.float64),\n",
    "            \"counts\": np.zeros(class_num, dtype=np.uint64)\n",
    "        },\n",
    "        \"quantification_prediction\": {\n",
    "            \"areas\": np.zeros(class_num, dtype=np.float64),\n",
    "            \"counts\": np.zeros(class_num, dtype=np.uint64)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Detection step\n",
    "    results[\"detect_prediction\"] = inference.detect(image)\n",
    "    \n",
    "    # Get patches\n",
    "    patched_images, patches = inference.patch(image, results[\"detect_prediction\"].boxes)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Classification step\n",
    "    results[\"classify_prediction\"], confidences = inference.classify(patched_images)\n",
    "    \n",
    "    # Get ground truth for classification\n",
    "    \n",
    "    results[\"classify_gt\"] = get_classify_ground_truth(\n",
    "        results[\"detect_prediction\"].boxes.xyxy,\n",
    "        detection_gt[image_name][:, :4],\n",
    "        detection_gt[image_name][:, 4],\n",
    "        config[\"bg_iou_thresh\"],\n",
    "    )\n",
    "    \n",
    "    # Merge detection and classification\n",
    "    results[\"inference_prediction\"] = inference.merge_detect_and_classification(\n",
    "        image,\n",
    "        results[\"detect_prediction\"].boxes.data,\n",
    "        results[\"classify_prediction\"],\n",
    "        confidences,\n",
    "    )\n",
    "        \n",
    "    # Save detection visualization\n",
    "    # results[\"inference_prediction\"].plot(\n",
    "    #     img=np.asarray(image)[..., ::-1],\n",
    "    #     filename=f\"{config['results']}/{image_name}.jpg\",\n",
    "    #     save=True,\n",
    "    #     line_width=5,\n",
    "    #     font_size=16,\n",
    "    # )\n",
    "    \n",
    "    # Quantify ground truth and predictions\n",
    "    results[\"quantification_gt\"] = quantify(\n",
    "        detection_gt[image_name][:, :4],\n",
    "        detection_gt[image_name][:, 4],\n",
    "        class_num,\n",
    "    )\n",
    "    \n",
    "    results[\"quantification_prediction\"] = quantify(\n",
    "        results[\"inference_prediction\"].boxes.xyxy,\n",
    "        results[\"inference_prediction\"].boxes.cls,\n",
    "            class_num,\n",
    "    )\n",
    "    \n",
    "    profiler_results = {\n",
    "        \"inference_detect\": inference.detect_profiler.dt,\n",
    "        \"inference_patch\": inference.patch_profiler.dt,\n",
    "        \"inference_classify\": inference.classify_profiler.dt,\n",
    "        \"inference_merge\": inference.merge_profiler.dt,\n",
    "        \"detector_resizer\": inference.detector.resizer_profiler.dt,\n",
    "        \"detector_patches\": inference.detector.patches_profiler.dt,\n",
    "        \"detector_yolo\": inference.detector.yolo_detector_profiler.dt,\n",
    "        \"detector_merge\": inference.detector.merge_predictions_profiler.dt,\n",
    "        \"detector_wbf\": inference.detector.wbf_profiler.dt,\n",
    "    }\n",
    "    return results, profiler_results\n",
    "\n",
    "# Process all images\n",
    "class_num = len(config[\"to_classes\"])\n",
    "image_results = []\n",
    "profiler_results = []\n",
    "image_loader = loader(config[\"val_source\"])\n",
    "\n",
    "for image_name, image in tqdm(image_loader):\n",
    "    im_res, profile_res = process_image(image_name, image, inference, detection_gt, config)\n",
    "    image_results.append(im_res)\n",
    "    profiler_results.append(profile_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491cf1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "def analyze_runtime(profiler_results):\n",
    "    \"\"\"\n",
    "    Analyze and visualize the runtime of different components in the detection pipeline.\n",
    "    \n",
    "    Args:\n",
    "        profiler_results: List of dictionaries containing timing information\n",
    "    \"\"\"\n",
    "    # Convert list of dictionaries to DataFrame\n",
    "    df = pd.DataFrame(profiler_results)\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    summary = df.describe()\n",
    "    \n",
    "    # Group operations by category\n",
    "    detector_cols = [col for col in df.columns if col.startswith('detector_')]\n",
    "    inference_cols = [col for col in df.columns if col.startswith('inference_')]\n",
    "    \n",
    "    # Create category dataframes\n",
    "    detector_df = df[detector_cols].copy()\n",
    "    inference_df = df[inference_cols].copy()\n",
    "    \n",
    "    # Add total time columns\n",
    "    detector_df['detector_total'] = detector_df.sum(axis=1)\n",
    "    inference_df['inference_total'] = inference_df.sum(axis=1)\n",
    "    \n",
    "    # Combine all data for full pipeline analysis\n",
    "    total_df = pd.DataFrame()\n",
    "    total_df['Detector'] = detector_df['detector_total']\n",
    "    total_df['Inference'] = inference_df['inference_total']\n",
    "    total_df['Total'] = total_df['Detector'] + total_df['Inference']\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"Runtime Analysis Summary (in seconds):\")\n",
    "    print(\"\\nDetector Operations:\")\n",
    "    for col in detector_cols:\n",
    "        print(f\"  {col.replace('detector_', '')}: {df[col].mean():.4f}s ± {df[col].std():.4f}s\")\n",
    "    print(f\"  Total detector time: {detector_df['detector_total'].mean():.4f}s\")\n",
    "    \n",
    "    print(\"\\nInference Operations:\")\n",
    "    for col in inference_cols:\n",
    "        print(f\"  {col.replace('inference_', '')}: {df[col].mean():.4f}s ± {df[col].std():.4f}s\")\n",
    "    print(f\"  Total inference time: {inference_df['inference_total'].mean():.4f}s\")\n",
    "    \n",
    "    print(f\"\\nTotal pipeline time: {total_df['Total'].mean():.4f}s ± {total_df['Total'].std():.4f}s\")\n",
    "    \n",
    "    # Calculate percentage of total time for each operation\n",
    "    total_time = df.sum(axis=1).mean()\n",
    "    percent_df = df.mean() / total_time * 100\n",
    "    \n",
    "    # Visualizations\n",
    "    # Set the style\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # 1. Box plot of all operations\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.title('Runtime Distribution of All Operations', fontsize=16)\n",
    "    sns.boxplot(data=df)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('all_operations_boxplot.png', dpi=300)\n",
    "    \n",
    "    # 2. Bar plot with mean runtime for each operation\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.title('Mean Runtime of Each Operation', fontsize=16)\n",
    "    sns.barplot(x=df.columns, y=df.mean(), palette='viridis')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('mean_runtime_barplot.png', dpi=300)\n",
    "    \n",
    "    # 3. Stacked bar for detector vs inference\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title('Proportion of Time: Detector vs Inference', fontsize=16)\n",
    "    \n",
    "    # Reshape data for easier plotting\n",
    "    detector_means = detector_df.drop('detector_total', axis=1).mean()\n",
    "    inference_means = inference_df.drop('inference_total', axis=1).mean()\n",
    "    \n",
    "    # Create stacked bars for detector operations\n",
    "    detector_data = pd.DataFrame({\n",
    "        'Operation': detector_means.index.str.replace('detector_', ''),\n",
    "        'Time': detector_means.values,\n",
    "        'Category': 'Detector'\n",
    "    })\n",
    "    \n",
    "    # Create stacked bars for inference operations\n",
    "    inference_data = pd.DataFrame({\n",
    "        'Operation': inference_means.index.str.replace('inference_', ''),\n",
    "        'Time': inference_means.values,\n",
    "        'Category': 'Inference'\n",
    "    })\n",
    "    \n",
    "    # Combine data\n",
    "    combined_data = pd.concat([detector_data, inference_data])\n",
    "    \n",
    "    # Plot stacked bars by category\n",
    "    sns.barplot(x='Category', y='Time', hue='Operation', data=combined_data, palette='viridis')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.legend(title='Operation', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('detector_vs_inference.png', dpi=300)\n",
    "    \n",
    "    # 4. Pie chart of time distribution\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.title('Time Distribution Across All Operations', fontsize=16)\n",
    "    \n",
    "    # Format labels with both operation name and percentage\n",
    "    def make_label(pct, values):\n",
    "        absolute = pct / 100. * sum(values)\n",
    "        return f\"{pct:.1f}%\\n({absolute:.3f}s)\"\n",
    "    \n",
    "    plt.pie(\n",
    "        percent_df, \n",
    "        labels=percent_df.index, \n",
    "        autopct=lambda pct: make_label(pct, percent_df.values),\n",
    "        startangle=90, \n",
    "        shadow=False, \n",
    "        explode=[0.05] * len(percent_df),\n",
    "        textprops={'fontsize': 9}\n",
    "    )\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('time_distribution_pie.png', dpi=300)\n",
    "    \n",
    "    # 5. Detailed breakdown with horizontal bars\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title('Detailed Runtime Breakdown (% of Total Pipeline)', fontsize=16)\n",
    "    \n",
    "    # Sort by percentage\n",
    "    sorted_percent = percent_df.sort_values(ascending=False)\n",
    "    \n",
    "    # Create color mapping - detector operations in blue shades, inference in green\n",
    "    colors = []\n",
    "    for op in sorted_percent.index:\n",
    "        if 'detector' in op:\n",
    "            colors.append('royalblue')\n",
    "        else:\n",
    "            colors.append('forestgreen')\n",
    "    \n",
    "    # Plot horizontal bars with percentage of total time\n",
    "    ax = sns.barplot(x=sorted_percent.values, y=sorted_percent.index, palette=colors)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, v in enumerate(sorted_percent.values):\n",
    "        ax.text(v + 0.5, i, f\"{v:.1f}%\", va='center')\n",
    "    \n",
    "    plt.xlabel('Percentage of Total Pipeline Time')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('detailed_time_breakdown.png', dpi=300)\n",
    "    \n",
    "    # 6. Time series plot showing runtime consistency across images\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.title('Runtime Consistency Across Images', fontsize=16)\n",
    "    \n",
    "    # Plot key operations over time\n",
    "    key_ops = ['detector_yolo', 'detector_wbf', 'inference_detect', 'inference_classify']\n",
    "    for op in key_ops:\n",
    "        plt.plot(df.index, df[op], label=op, marker='o', alpha=0.7, markersize=4)\n",
    "    \n",
    "    plt.xlabel('Image Index')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('runtime_consistency.png', dpi=300)\n",
    "    \n",
    "    # Return the processed dataframes for further analysis if needed\n",
    "    return {\n",
    "        'raw_data': df,\n",
    "        'detector_data': detector_df,\n",
    "        'inference_data': inference_df,\n",
    "        'total_data': total_df\n",
    "    }\n",
    "\n",
    "\n",
    "results = analyze_runtime(profiler_results)  # Use your actual profiler_results list from your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0831fe",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Structure results for evaluation"
   },
   "outputs": [],
   "source": [
    "def structure_results(image_results, class_num):\n",
    "    \"\"\"Organize image-level results into structured collections for evaluation.\"\"\"\n",
    "    detect_predictions = {}\n",
    "    classify_predictions = {}\n",
    "    classify_gt = {}\n",
    "    inference_predictions = {}\n",
    "    quantification_gt = {}\n",
    "    quantification_predictions = {}\n",
    "    \n",
    "    classify_predictions_flat = []\n",
    "    classify_gt_flat = []\n",
    "    \n",
    "    # Process each image result\n",
    "    for result in image_results:\n",
    "        image_name = result[\"image_name\"]\n",
    "        \n",
    "        # Store detection and inference predictions\n",
    "        detect_predictions[image_name] = result[\"detect_prediction\"]\n",
    "        \n",
    "        # Store classification data\n",
    "        classify_predictions[image_name] = result[\"classify_prediction\"]\n",
    "        classify_gt[image_name] = result[\"classify_gt\"]\n",
    "        \n",
    "        # Extend flat lists for classification\n",
    "        classify_predictions_flat.extend(result[\"classify_prediction\"])\n",
    "        classify_gt_flat.extend(result[\"classify_gt\"])\n",
    "        \n",
    "        # Store inference predictions\n",
    "        inference_predictions[image_name] = result[\"inference_prediction\"]\n",
    "        \n",
    "        # Store quantification data\n",
    "        quantification_gt[image_name] = result[\"quantification_gt\"]\n",
    "        quantification_predictions[image_name] = result[\"quantification_prediction\"]\n",
    "    \n",
    "    # Create flattened quantification arrays\n",
    "    num_images = len(quantification_gt)\n",
    "    \n",
    "    quantification_gt_flat = {\n",
    "        \"areas\": np.zeros((num_images, class_num), dtype=np.float64),\n",
    "        \"counts\": np.zeros((num_images, class_num), dtype=np.uint64),\n",
    "    }\n",
    "    \n",
    "    quantification_predictions_flat = {\n",
    "        \"areas\": np.zeros((num_images, class_num), dtype=np.float64),\n",
    "        \"counts\": np.zeros((num_images, class_num), dtype=np.uint64),\n",
    "    }\n",
    "    \n",
    "    # Fill quantification arrays\n",
    "    for idx, image_name in enumerate(quantification_gt.keys()):\n",
    "        gt = quantification_gt[image_name]\n",
    "        pred = quantification_predictions[image_name]\n",
    "        \n",
    "        quantification_gt_flat[\"areas\"][idx] = gt[\"areas\"]\n",
    "        quantification_gt_flat[\"counts\"][idx] = gt[\"counts\"]\n",
    "        quantification_predictions_flat[\"areas\"][idx] = pred[\"areas\"]\n",
    "        quantification_predictions_flat[\"counts\"][idx] = pred[\"counts\"]\n",
    "    \n",
    "    return {\n",
    "        \"detect_predictions\": detect_predictions,\n",
    "        \"classify_predictions\": classify_predictions,\n",
    "        \"classify_gt\": classify_gt,\n",
    "        \"inference_predictions\": inference_predictions,\n",
    "        \"quantification_gt\": quantification_gt,\n",
    "        \"quantification_predictions\": quantification_predictions,\n",
    "        \"classify_predictions_flat\": classify_predictions_flat,\n",
    "        \"classify_gt_flat\": classify_gt_flat,\n",
    "        \"quantification_gt_flat\": quantification_gt_flat,\n",
    "        \"quantification_predictions_flat\": quantification_predictions_flat\n",
    "    }\n",
    "\n",
    "# Structure all results\n",
    "all_results = structure_results(image_results, class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbaa0a7",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Detection evaluation"
   },
   "outputs": [],
   "source": [
    "def plot_detection_metrics(results, title_prefix=\"\"):\n",
    "    \"\"\"Plot detection metrics in an organized way.\"\"\"\n",
    "    # Separate AP and AR metrics for better visualization\n",
    "    ap_metrics = {k: v for k, v in results.items() if k.startswith(\"AP\")}\n",
    "    ar_metrics = {k: v for k, v in results.items() if k.startswith(\"AR\")}\n",
    "    \n",
    "    # Plot AP metrics\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(x=list(ap_metrics.keys()), y=list(ap_metrics.values()), palette=\"coolwarm\")\n",
    "    plt.title(f\"{title_prefix}Average Precision (AP) Metrics\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot AR metrics\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(x=list(ar_metrics.keys()), y=list(ar_metrics.values()), palette=\"viridis\")\n",
    "    plt.title(f\"{title_prefix}Average Recall (AR) Metrics\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate detection results\n",
    "detect_results = coco_detection_evaluator.eval(\n",
    "    all_results[\"detect_predictions\"], names_to_ids, useCats=False\n",
    ")\n",
    "\n",
    "with open(os.path.join(config['results'], 'detect_coco.json'), \"w\") as f:\n",
    "    json.dump(detect_results, f, indent=4)\n",
    "\n",
    "# Plot detection metrics\n",
    "plot_detection_metrics(detect_results, \"Detection: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd14c84",
   "metadata": {
    "title": "Classification evaluation"
   },
   "outputs": [],
   "source": [
    "def evaluate_classification(predictions, ground_truth, class_names):\n",
    "    \"\"\"Evaluate classification metrics and plot results.\"\"\"\n",
    "    precision = precision_score(ground_truth, predictions, average='macro')\n",
    "    recall = recall_score(ground_truth, predictions, average='macro')\n",
    "    f1 = f1_score(ground_truth, predictions, average='macro')\n",
    "    accuracy = accuracy_score(ground_truth, predictions)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(ground_truth, predictions)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=list(class_names.values()),\n",
    "                yticklabels=list(class_names.values()))\n",
    "    plt.title(f'Confusion Matrix for Classification')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot metrics\n",
    "    metrics = {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1,\n",
    "        \"Accuracy\": accuracy,\n",
    "    }\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(x=list(metrics.keys()), y=list(metrics.values()), palette=\"viridis\")\n",
    "    plt.title(\"Classification Metrics\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Evaluate classification results\n",
    "classification_metrics = evaluate_classification(\n",
    "    all_results[\"classify_predictions_flat\"], \n",
    "    all_results[\"classify_gt_flat\"],\n",
    "    config[\"to_classes\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302bf595",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Full inference evaluation"
   },
   "outputs": [],
   "source": [
    "infere_results = coco_detection_evaluator.eval(\n",
    "    all_results[\"inference_predictions\"], names_to_ids, useCats=True\n",
    ")\n",
    "\n",
    "with open(os.path.join(config['results'], 'inference_coco.json'), \"w\") as f:\n",
    "    json.dump(infere_results, f, indent=4)\n",
    "\n",
    "# Plot inference metrics\n",
    "plot_detection_metrics(infere_results, \"Full Inference: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8954021a",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Quantification evaluation functions"
   },
   "outputs": [],
   "source": [
    "def evaluate_quantification_metrics(gt_values, pred_values, class_names, metric_type=\"Area\"):\n",
    "    \"\"\"Calculate and return quantification metrics.\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Per-class metrics\n",
    "    for class_id, class_name in class_names.items():\n",
    "        if class_id != 0:  # Skip background\n",
    "            gt = gt_values[:, class_id]\n",
    "            pred = pred_values[:, class_id]\n",
    "            \n",
    "            # Skip if no ground truth\n",
    "            if np.sum(gt) == 0:\n",
    "                continue\n",
    "                \n",
    "            metrics[class_name] = {\n",
    "                \"Explained Variance\": explained_variance_score(gt, pred),\n",
    "                #\"MAE\": mean_absolute_error(gt, pred),\n",
    "                \"R2\": r2_score(gt, pred),\n",
    "              #  \"MAPE\": mean_absolute_percentage_error(gt, pred) if np.any(gt != 0) else float('nan'),\n",
    "            }\n",
    "    \n",
    "    # Overall metrics (excluding background)\n",
    "    all_gt = gt_values[:, 1:].sum(axis=1)\n",
    "    all_pred = pred_values[:, 1:].sum(axis=1)\n",
    "    \n",
    "    if np.sum(all_gt) > 0:\n",
    "        metrics[\"All Classes\"] = {\n",
    "            \"Explained Variance\": explained_variance_score(all_gt, all_pred),\n",
    "            #\"MAE\": mean_absolute_error(all_gt, all_pred),\n",
    "            \"R2\": r2_score(all_gt, all_pred),\n",
    "           # \"MAPE\": mean_absolute_percentage_error(all_gt, all_pred) if np.any(all_gt != 0) else float('nan'),\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def plot_quantification_metrics(metrics, title_prefix=\"\"):\n",
    "    \"\"\"Plot quantification metrics for each class.\"\"\"\n",
    "    for name, class_metrics in metrics.items():\n",
    "        valid_metrics = {k: v for k, v in class_metrics.items() if not np.isnan(v)}\n",
    "        \n",
    "        if not valid_metrics:\n",
    "            continue\n",
    "            \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.barplot(x=list(valid_metrics.keys()), y=list(valid_metrics.values()))\n",
    "        plt.title(f\"{title_prefix} Metrics for {name}\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9382a4",
   "metadata": {
    "title": "Evaluate area metrics"
   },
   "outputs": [],
   "source": [
    "area_metrics = evaluate_quantification_metrics(\n",
    "    all_results[\"quantification_gt_flat\"][\"areas\"],\n",
    "    all_results[\"quantification_predictions_flat\"][\"areas\"],\n",
    "    config[\"to_classes\"],\n",
    "    \"Area\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "plot_quantification_metrics(area_metrics, \"Area\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50926e09",
   "metadata": {
    "title": "Evaluate count metrics"
   },
   "outputs": [],
   "source": [
    "count_metrics = evaluate_quantification_metrics(\n",
    "    all_results[\"quantification_gt_flat\"][\"counts\"],\n",
    "    all_results[\"quantification_predictions_flat\"][\"counts\"],\n",
    "    config[\"to_classes\"],\n",
    "    \"Count\"\n",
    ")\n",
    "\n",
    "plot_quantification_metrics(count_metrics, \"Count\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
