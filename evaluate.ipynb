{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae749deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from functools import partial\n",
    "from ultralytics.engine.results import Results\n",
    "from tqdm import tqdm\n",
    "\n",
    "from inference import Detector, Classificator, Inference\n",
    "from inference.Inference import quantify \n",
    "\n",
    "# Object detection\n",
    "from utils.data import as_coco\n",
    "from utils.data.load import load_yolo, loader\n",
    "from utils.metrics.coco import CocoEvaluator\n",
    "from utils.metrics.utils import get_classify_ground_truth\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    explained_variance_score,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    mean_absolute_percentage_error\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c603460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"val_source\": \"../datasets/orobanche_cummana/autosplit_val.txt\",\n",
    "    \"detect_model_path\": os.path.normpath(\"./runs/detect/train2/weights/best.pt\"),\n",
    "    \"classify_model_path\": os.path.normpath(\"./runs/classify/train/weights/best.pt\"),\n",
    "    'results': 'results',\n",
    "    \n",
    "    \"classes\": {\n",
    "      0: \"background\",\n",
    "      1: \"healthy\",\n",
    "      2: \"necrotic\",  \n",
    "    },\n",
    "    #DETECTION\n",
    "    \"device\": 0,\n",
    "    \"overlap\": 0.2,\n",
    "    \"patch_size\": 1024,\n",
    "    \"size_factor\": 1.0,\n",
    "    \"conf_thresh\": 0.25,\n",
    "    \"nms_iou_thresh\":  0.7,\n",
    "    \"max_patch_detections\": 300,\n",
    "    \"patch_per_batch\": 4,\n",
    "    \"pre_wbf_detections\":  3000,\n",
    "    \"wbf_ios_thresh\": 0.5,\n",
    "    \"max_detections\": 1000,\n",
    "    \n",
    "    #CLASSIFICATION (INFERENCE)\n",
    "    \"offset\": 0.2,\n",
    "    \"classify_img_size\": 224,\n",
    "    \n",
    "    #CLASSIFICATION (EVALUATION)\n",
    "    \"bg_iou_thresh\": 0.7,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d4fea2",
   "metadata": {
    "title": "Load ground truth data and setup evaluator"
   },
   "outputs": [],
   "source": [
    "detection_gt = {k: np.asarray(v) for k, v in load_yolo(config[\"val_source\"]).items()}\n",
    "coco_gt_detection, names_to_ids = as_coco(detection_gt, config[\"classes\"])\n",
    "coco_detection_evaluator = CocoEvaluator(coco_gt_detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7643de4a",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Initialize models"
   },
   "outputs": [],
   "source": [
    "detect = Detector(\n",
    "    model_path=config[\"detect_model_path\"],\n",
    "    device=config[\"device\"],\n",
    "    overlap=config[\"overlap\"],\n",
    "    patch_size=config[\"patch_size\"],\n",
    "    size_factor=config[\"size_factor\"],\n",
    "    conf_thresh=config[\"conf_thresh\"],\n",
    "    nms_iou_thresh=config[\"nms_iou_thresh\"],\n",
    "    max_patch_detections=config[\"max_patch_detections\"],\n",
    "    patch_per_batch=config[\"patch_per_batch\"],\n",
    "    pre_wbf_detections=config[\"pre_wbf_detections\"],\n",
    "    wbf_ios_thresh=config[\"wbf_ios_thresh\"],\n",
    "    max_detections=config[\"max_detections\"],\n",
    "    single_cls=True,\n",
    ")\n",
    "\n",
    "classify = Classificator(\n",
    "    model_path=config[\"classify_model_path\"], \n",
    "    device=config[\"device\"], \n",
    "    img_size=config[\"classify_img_size\"],\n",
    ")\n",
    "\n",
    "inference = Inference(detect, classify, offset=config['offset'])\n",
    "os.makedirs(config['results'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a375bad0",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Process predictions and ground truth"
   },
   "outputs": [],
   "source": [
    "def process_image(image_name, image, inference, detection_gt, config):\n",
    "    \"\"\"Process a single image through the detection and classification pipeline.\"\"\"\n",
    "    class_num = len(config[\"classes\"])\n",
    "    results = {\n",
    "        \"image_name\": image_name,\n",
    "        \"detect_prediction\": None,\n",
    "        \"classify_prediction\": [],\n",
    "        \"classify_gt\": [],\n",
    "        \"inference_prediction\": {},\n",
    "        \"quantification_gt\": {\n",
    "            \"areas\": np.zeros(class_num, dtype=np.float64),\n",
    "            \"counts\": np.zeros(class_num, dtype=np.uint64)\n",
    "        },\n",
    "        \"quantification_prediction\": {\n",
    "            \"areas\": np.zeros(class_num, dtype=np.float64),\n",
    "            \"counts\": np.zeros(class_num, dtype=np.uint64)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Detection step\n",
    "    results[\"detect_prediction\"] = inference.detect(image)\n",
    "    \n",
    "    # Get patches\n",
    "    patched_images, patches = inference.patch(image, results[\"detect_prediction\"].boxes)\n",
    "    \n",
    "    if patches:\n",
    "        # Save detection visualization\n",
    "        results[\"detect_prediction\"].plot(\n",
    "            img=np.asarray(image)[..., ::-1],\n",
    "            filename=f\"{config['results']}/{image_name}.jpg\",\n",
    "            save=True,\n",
    "            line_width=5,\n",
    "            font_size=16,\n",
    "        )\n",
    "        \n",
    "        # Classification step\n",
    "        results[\"classify_prediction\"], confidences = inference.classify(patched_images)\n",
    "        \n",
    "        # Get ground truth for classification\n",
    "        results[\"classify_gt\"] = get_classify_ground_truth(\n",
    "            results[\"detect_prediction\"].boxes.xyxy,\n",
    "            detection_gt[image_name][:, :4],\n",
    "            detection_gt[image_name][:, 4],\n",
    "            config[\"bg_iou_thresh\"],\n",
    "        )\n",
    "        \n",
    "        # Merge detection and classification\n",
    "        results[\"inference_prediction\"] = inference.merge_detect_and_classification(\n",
    "            image,\n",
    "            results[\"detect_prediction\"].boxes.data,\n",
    "            results[\"classify_prediction\"],\n",
    "            confidences,\n",
    "        )\n",
    "        \n",
    "        # Quantify ground truth and predictions\n",
    "        results[\"quantification_gt\"] = quantify(\n",
    "            detection_gt[image_name][:, :4],\n",
    "            detection_gt[image_name][:, 4],\n",
    "            class_num,\n",
    "        )\n",
    "        \n",
    "        results[\"quantification_prediction\"] = quantify(\n",
    "            results[\"inference_prediction\"].boxes.xyxy,\n",
    "            results[\"inference_prediction\"].boxes.cls,\n",
    "            class_num,\n",
    "        )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Process all images\n",
    "class_num = len(config[\"classes\"])\n",
    "image_results = []\n",
    "image_loader = loader(config[\"val_source\"])\n",
    "\n",
    "for image_name, image in tqdm(image_loader):\n",
    "    image_results.append(process_image(image_name, image, inference, detection_gt, config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0831fe",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Structure results for evaluation"
   },
   "outputs": [],
   "source": [
    "def structure_results(image_results, class_num):\n",
    "    \"\"\"Organize image-level results into structured collections for evaluation.\"\"\"\n",
    "    detect_predictions = {}\n",
    "    classify_predictions = {}\n",
    "    classify_gt = {}\n",
    "    inference_predictions = {}\n",
    "    quantification_gt = {}\n",
    "    quantification_predictions = {}\n",
    "    \n",
    "    classify_predictions_flat = []\n",
    "    classify_gt_flat = []\n",
    "    \n",
    "    # Process each image result\n",
    "    for result in image_results:\n",
    "        image_name = result[\"image_name\"]\n",
    "        \n",
    "        # Store detection and inference predictions\n",
    "        detect_predictions[image_name] = result[\"detect_prediction\"]\n",
    "        \n",
    "        # Store classification data\n",
    "        classify_predictions[image_name] = result[\"classify_prediction\"]\n",
    "        classify_gt[image_name] = result[\"classify_gt\"]\n",
    "        \n",
    "        # Extend flat lists for classification\n",
    "        classify_predictions_flat.extend(result[\"classify_prediction\"])\n",
    "        classify_gt_flat.extend(result[\"classify_gt\"])\n",
    "        \n",
    "        # Store inference predictions\n",
    "        inference_predictions[image_name] = result[\"inference_prediction\"]\n",
    "        \n",
    "        # Store quantification data\n",
    "        quantification_gt[image_name] = result[\"quantification_gt\"]\n",
    "        quantification_predictions[image_name] = result[\"quantification_prediction\"]\n",
    "    \n",
    "    # Create flattened quantification arrays\n",
    "    num_images = len(quantification_gt)\n",
    "    \n",
    "    quantification_gt_flat = {\n",
    "        \"areas\": np.zeros((num_images, class_num), dtype=np.float64),\n",
    "        \"counts\": np.zeros((num_images, class_num), dtype=np.uint64),\n",
    "    }\n",
    "    \n",
    "    quantification_predictions_flat = {\n",
    "        \"areas\": np.zeros((num_images, class_num), dtype=np.float64),\n",
    "        \"counts\": np.zeros((num_images, class_num), dtype=np.uint64),\n",
    "    }\n",
    "    \n",
    "    # Fill quantification arrays\n",
    "    for idx, image_name in enumerate(quantification_gt.keys()):\n",
    "        gt = quantification_gt[image_name]\n",
    "        pred = quantification_predictions[image_name]\n",
    "        \n",
    "        quantification_gt_flat[\"areas\"][idx] = gt[\"areas\"]\n",
    "        quantification_gt_flat[\"counts\"][idx] = gt[\"counts\"]\n",
    "        quantification_predictions_flat[\"areas\"][idx] = pred[\"areas\"]\n",
    "        quantification_predictions_flat[\"counts\"][idx] = pred[\"counts\"]\n",
    "    \n",
    "    return {\n",
    "        \"detect_predictions\": detect_predictions,\n",
    "        \"classify_predictions\": classify_predictions,\n",
    "        \"classify_gt\": classify_gt,\n",
    "        \"inference_predictions\": inference_predictions,\n",
    "        \"quantification_gt\": quantification_gt,\n",
    "        \"quantification_predictions\": quantification_predictions,\n",
    "        \"classify_predictions_flat\": classify_predictions_flat,\n",
    "        \"classify_gt_flat\": classify_gt_flat,\n",
    "        \"quantification_gt_flat\": quantification_gt_flat,\n",
    "        \"quantification_predictions_flat\": quantification_predictions_flat\n",
    "    }\n",
    "\n",
    "# Structure all results\n",
    "all_results = structure_results(image_results, class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbaa0a7",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Detection evaluation"
   },
   "outputs": [],
   "source": [
    "def plot_detection_metrics(results, title_prefix=\"\"):\n",
    "    \"\"\"Plot detection metrics in an organized way.\"\"\"\n",
    "    # Separate AP and AR metrics for better visualization\n",
    "    ap_metrics = {k: v for k, v in results.items() if k.startswith(\"AP\")}\n",
    "    ar_metrics = {k: v for k, v in results.items() if k.startswith(\"AR\")}\n",
    "    \n",
    "    # Plot AP metrics\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(x=list(ap_metrics.keys()), y=list(ap_metrics.values()), palette=\"coolwarm\")\n",
    "    plt.title(f\"{title_prefix}Average Precision (AP) Metrics\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot AR metrics\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(x=list(ar_metrics.keys()), y=list(ar_metrics.values()), palette=\"viridis\")\n",
    "    plt.title(f\"{title_prefix}Average Recall (AR) Metrics\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate detection results\n",
    "detect_results = coco_detection_evaluator.eval(\n",
    "    all_results[\"detect_predictions\"], names_to_ids, useCats=False\n",
    ")\n",
    "\n",
    "with open(os.path.join(config['results'], 'detect_coco.json'), \"w\") as f:\n",
    "    json.dump(detect_results, f, indent=4)\n",
    "\n",
    "# Plot detection metrics\n",
    "plot_detection_metrics(detect_results, \"Detection: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd14c84",
   "metadata": {
    "title": "Classification evaluation"
   },
   "outputs": [],
   "source": [
    "def evaluate_classification(predictions, ground_truth, class_names):\n",
    "    \"\"\"Evaluate classification metrics and plot results.\"\"\"\n",
    "    precision = precision_score(ground_truth, predictions, average='macro')\n",
    "    recall = recall_score(ground_truth, predictions, average='macro')\n",
    "    f1 = f1_score(ground_truth, predictions, average='macro')\n",
    "    accuracy = accuracy_score(ground_truth, predictions)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(ground_truth, predictions)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=list(class_names.values()),\n",
    "                yticklabels=list(class_names.values()))\n",
    "    plt.title(f'Confusion Matrix for Classification')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot metrics\n",
    "    metrics = {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1,\n",
    "        \"Accuracy\": accuracy,\n",
    "    }\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(x=list(metrics.keys()), y=list(metrics.values()), palette=\"viridis\")\n",
    "    plt.title(\"Classification Metrics\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Evaluate classification results\n",
    "classification_metrics = evaluate_classification(\n",
    "    all_results[\"classify_predictions_flat\"], \n",
    "    all_results[\"classify_gt_flat\"],\n",
    "    config[\"classes\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302bf595",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Full inference evaluation"
   },
   "outputs": [],
   "source": [
    "infere_results = coco_detection_evaluator.eval(\n",
    "    all_results[\"inference_predictions\"], names_to_ids, useCats=False\n",
    ")\n",
    "\n",
    "with open(os.path.join(config['results'], 'inference_coco.json'), \"w\") as f:\n",
    "    json.dump(infere_results, f, indent=4)\n",
    "\n",
    "# Plot inference metrics\n",
    "plot_detection_metrics(infere_results, \"Full Inference: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8954021a",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Quantification evaluation functions"
   },
   "outputs": [],
   "source": [
    "def evaluate_quantification_metrics(gt_values, pred_values, class_names, metric_type=\"Area\"):\n",
    "    \"\"\"Calculate and return quantification metrics.\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Per-class metrics\n",
    "    for class_id, class_name in class_names.items():\n",
    "        if class_id != 0:  # Skip background\n",
    "            gt = gt_values[:, class_id]\n",
    "            pred = pred_values[:, class_id]\n",
    "            \n",
    "            # Skip if no ground truth\n",
    "            if np.sum(gt) == 0:\n",
    "                continue\n",
    "                \n",
    "            metrics[class_name] = {\n",
    "                \"Explained Variance\": explained_variance_score(gt, pred),\n",
    "                \"MAE\": mean_absolute_error(gt, pred),\n",
    "                \"R2\": r2_score(gt, pred),\n",
    "                \"MAPE\": mean_absolute_percentage_error(gt, pred) if np.any(gt != 0) else float('nan'),\n",
    "            }\n",
    "    \n",
    "    # Overall metrics (excluding background)\n",
    "    all_gt = gt_values[:, 1:].sum(axis=1)\n",
    "    all_pred = pred_values[:, 1:].sum(axis=1)\n",
    "    \n",
    "    if np.sum(all_gt) > 0:\n",
    "        metrics[\"All Classes\"] = {\n",
    "            \"Explained Variance\": explained_variance_score(all_gt, all_pred),\n",
    "            \"MAE\": mean_absolute_error(all_gt, all_pred),\n",
    "            \"R2\": r2_score(all_gt, all_pred),\n",
    "            \"MAPE\": mean_absolute_percentage_error(all_gt, all_pred) if np.any(all_gt != 0) else float('nan'),\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def plot_quantification_metrics(metrics, title_prefix=\"\"):\n",
    "    \"\"\"Plot quantification metrics for each class.\"\"\"\n",
    "    for name, class_metrics in metrics.items():\n",
    "        valid_metrics = {k: v for k, v in class_metrics.items() if not np.isnan(v)}\n",
    "        \n",
    "        if not valid_metrics:\n",
    "            continue\n",
    "            \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.barplot(x=list(valid_metrics.keys()), y=list(valid_metrics.values()))\n",
    "        plt.title(f\"{title_prefix} Metrics for {name}\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9382a4",
   "metadata": {
    "title": "Evaluate area metrics"
   },
   "outputs": [],
   "source": [
    "area_metrics = evaluate_quantification_metrics(\n",
    "    all_results[\"quantification_gt_flat\"][\"areas\"],\n",
    "    all_results[\"quantification_predictions_flat\"][\"areas\"],\n",
    "    config[\"classes\"],\n",
    "    \"Area\"\n",
    ")\n",
    "\n",
    "plot_quantification_metrics(area_metrics, \"Area\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50926e09",
   "metadata": {
    "title": "Evaluate count metrics"
   },
   "outputs": [],
   "source": [
    "count_metrics = evaluate_quantification_metrics(\n",
    "    all_results[\"quantification_gt_flat\"][\"counts\"],\n",
    "    all_results[\"quantification_predictions_flat\"][\"counts\"],\n",
    "    config[\"classes\"],\n",
    "    \"Count\"\n",
    ")\n",
    "\n",
    "plot_quantification_metrics(count_metrics, \"Count\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
