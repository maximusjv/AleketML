{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKXbvPtmIAzW"
   },
   "source": [
    "# Aleket Faster R-CNN training notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pillow\n",
    "%pip install numpy<2.0\n",
    "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cu124\n",
    "%pip install matplotlib\n",
    "%pip install gdown\n",
    "%pip install tqdm\n",
    "\n",
    "from IPython.display import clear_output\n",
    "clear_output(wait=False)\n",
    "\n",
    "print(\"ALL DEPENDENCIES INSTALLED\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T09:04:42.063720Z",
     "start_time": "2024-10-23T09:04:22.868274Z"
    },
    "executionInfo": {
     "elapsed": 7480,
     "status": "ok",
     "timestamp": 1726688885304,
     "user": {
      "displayName": "maximus JV",
      "userId": "01037924598235015782"
     },
     "user_tz": -180
    },
    "id": "QYh3of1UgFZs"
   },
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "# Standard Library\n",
    "import os\n",
    "\n",
    "# Third-Party Libraries\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.axes import Axes\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "\n",
    "# Utils\n",
    "from aleket_dataset import AleketDataset, download_dataset, split_dataset\n",
    "from utils import get_model, load_checkpoint\n",
    "from training_and_evaluation import train\n",
    "from dataset_statisics import visualize_samples, count_analyze\n",
    "from run_params import RunParams, parse_params\n",
    "from predictor import Predictor\n",
    "from metrics import Evaluator, VALIDATION_METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T09:04:42.389081Z",
     "start_time": "2024-10-23T09:04:42.120733Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def augment_example(ds):\n",
    "    examples = visualize_samples(ds, image_ids_to_visualize=list(range(4)))\n",
    "    fig=plt.figure(figsize=(40, 10))\n",
    "    columns = 4\n",
    "    rows = 1\n",
    "    for i in range(1, columns*rows +1):\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        plt.imshow(examples[i-1])\n",
    "    plt.show()\n",
    "\n",
    "def draw_heat_map(name: str, values: np.ndarray, ax: Axes, x_ticks: np.ndarray, y_ticks: np.ndarray):\n",
    "\n",
    "    masked_results = np.ma.masked_where(values == -1, values)\n",
    "    ax.imshow(masked_results, cmap='viridis', vmin=0, interpolation='nearest')\n",
    "\n",
    "    X = len(x_ticks)\n",
    "    Y = len(y_ticks)\n",
    "\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel('Score Threshold')\n",
    "    ax.set_ylabel('NMS Threshold')\n",
    "    ax.set_xticks(np.arange(X))\n",
    "    ax.set_yticks(np.arange(Y))\n",
    "    ax.set_xticklabels(x_ticks)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "    max_val = values.max()\n",
    "    \n",
    "    for i in range(Y):\n",
    "        for j in range(X):\n",
    "            value = values[i, j]\n",
    "            color = 'black' if value > max_val/2 else 'white'\n",
    "            text = ax.text(j, i, f'{value:.3f}', ha=\"center\", va=\"center\", color=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device Selection\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Random Seed for Dataset split\n",
    "SEED = 1\n",
    "np_generator = np.random.default_rng(SEED)\n",
    "\n",
    "# Dataset split\n",
    "DATASET_FRACTION = 1\n",
    "VALIDATION_FRACTION = 0.2\n",
    "dataset = AleketDataset(download_dataset(\"dataset_patched\", \"\"))\n",
    "full_dataset = AleketDataset(download_dataset(\"dataset_full_images\", \"\"))\n",
    "train_set, val_set = split_dataset(dataset, DATASET_FRACTION, VALIDATION_FRACTION, np_generator)\n",
    "\n",
    "# Model\n",
    "model = get_model(device, trainable_backbone_layers=3)\n",
    "\n",
    "print(f\"Using model: {model._get_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_NAME = \"ac_run6_v2_tb=5\"\n",
    "\n",
    "TRAIN_COMPLETE = False\n",
    "params = RunParams(\n",
    "    run_name=RUN_NAME,\n",
    "    batch_size=4,\n",
    "    dataloader_workers=4, \n",
    "    total_epochs=150,\n",
    "    augmentation={  \n",
    "        \"horizontal_flip\": {\n",
    "            \"p\": 0.5\n",
    "        },\n",
    "        \"vertical_flip\": {\n",
    "            \"p\": 0.5\n",
    "        },\n",
    "        \"scale_jitter\": {\n",
    "            \"target_size\": (1024, 1024),\n",
    "            \"scale_range\": (0.5, 1.3)\n",
    "        },\n",
    "    },\n",
    "    optimizer={\n",
    "        \"lr\": 0.005,\n",
    "        \"weight_decay\": 0.00009\n",
    "    },\n",
    "    lr_scheduler={\n",
    "        \"factor\": 0.1,\n",
    "        \"patience\": 15,\n",
    "        \"min_lr\": 0.0001\n",
    "    },\n",
    "    validation_set=val_set,\n",
    "    train_set=train_set\n",
    "    )\n",
    "\n",
    "\n",
    "print(f\"Train parameters for '{RUN_NAME}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_params = parse_params(params, model, dataset)\n",
    "train_dataloader = parsed_params[\"train_loader\"]\n",
    "val_dataloader = parsed_params[\"val_loader\"]\n",
    "augmentation = parsed_params[\"augmentation\"]\n",
    "\n",
    "count_analyze(full_dataset, save_folder=\"full_dataset_statistics\")\n",
    "count_analyze(dataset, save_folder=\"patched_dataset_statistics\")\n",
    "count_analyze(dataset, indices=train_dataloader.dataset.indices, save_folder=\"patched_train_dataset_statistics\")\n",
    "count_analyze(dataset, indices=val_dataloader.dataset.indices, save_folder=\"patched_val_dataset_statistics\")\n",
    "\n",
    "dataset.augmentation = augmentation\n",
    "augment_example(train_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T09:16:28.489606Z",
     "start_time": "2024-10-23T09:04:42.842532Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G9UdbXiJIAzZ",
    "outputId": "4eaac050-8192-4ffe-e5ae-ee59701ce7b2"
   },
   "outputs": [],
   "source": [
    "train(model, dataset, params, device, checkpoints=True)\n",
    "clear_output(wait=False)\n",
    "print(\"TRAIN COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = RunParams()\n",
    "params.load(os.path.join('results', RUN_NAME, \"params.json\")) # override parameters\n",
    "train(model, dataset, params, device, checkpoints=True, resume=True, verbose=True)\n",
    "clear_output(wait=False)\n",
    "print(\"TRAIN COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_NAME_TO_TEST = RUN_NAME\n",
    "\n",
    "\n",
    "run_dir = os.path.join(\"results\", RUN_NAME_TO_TEST)\n",
    "params_path = os.path.join(run_dir, \"params.json\")\n",
    "checkpoint_path = os.path.join(run_dir,\"checkpoints\", \"best.pth\")\n",
    "\n",
    "model = get_model(device)\n",
    "params = RunParams()\n",
    "params.load(params_path)\n",
    "\n",
    "val_indices = full_dataset.to_indices(params.validation_set.keys())\n",
    "model = load_checkpoint(model, checkpoint_path)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor(\n",
    "    model,\n",
    "    device,\n",
    "    detections_per_patch=150,\n",
    "    detections_per_image=300,\n",
    "    images_per_batch=2,\n",
    "    image_size_factor=1,\n",
    "    patches_per_batch=8,\n",
    ")\n",
    "\n",
    "nms_thrs = np.round(np.flip(np.arange(0.2, 0.5 + 1e-3, 0.1)),2)\n",
    "score_thrs = np.round(np.arange(0.1, 0.8 + 1e-4, 0.05),2)\n",
    "\n",
    "np.savetxt(os.path.join(run_dir,\"nms_thrs.csv\"), nms_thrs, delimiter=',', fmt='%.2f')\n",
    "np.savetxt(os.path.join(run_dir,\"score_thrs.csv\"), score_thrs, delimiter=',', fmt='%.2f')\n",
    "\n",
    "N = len(nms_thrs)\n",
    "S = len(score_thrs)\n",
    "\n",
    "eval = Evaluator(full_dataset, val_indices,)\n",
    "\n",
    "results_ap  = np.full((N,S), -1.0)\n",
    "results_aad = np.full((N,S), -1.0)\n",
    "results_acd = np.full((N,S), -1.0)\n",
    "\n",
    "for i, n in enumerate(nms_thrs):\n",
    "    for j, s in tqdm(enumerate(score_thrs), total=S):\n",
    "        try:\n",
    "            stats = predictor.eval_dataset(full_dataset, val_indices, n, s, eval)\n",
    "            results_ap[i, j] = stats[VALIDATION_METRICS[0]]\n",
    "            results_acd[i, j] = stats[VALIDATION_METRICS[-2]]\n",
    "            results_aad[i, j] = stats[VALIDATION_METRICS[-1]]\n",
    "        except Exception as e:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(os.path.join(run_dir,\"ap_analysis.csv\"), results_ap, delimiter=',', fmt='%.4f')\n",
    "np.savetxt(os.path.join(run_dir,\"aad_analysis.csv\"), results_aad, delimiter=',', fmt='%.4f')\n",
    "np.savetxt(os.path.join(run_dir,\"acd_analysis.csv\"), results_acd, delimiter=',', fmt='%.4f')\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(60, 10))\n",
    "\n",
    "draw_heat_map(\"AP\",  results_ap, axes[0], score_thrs, nms_thrs)\n",
    "draw_heat_map(\"AAD\", results_aad, axes[1],score_thrs, nms_thrs)\n",
    "draw_heat_map(\"ACD\", results_acd, axes[2],score_thrs, nms_thrs)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor(\n",
    "    model,\n",
    "    device,\n",
    "    detections_per_patch=150,\n",
    "    detections_per_image=300,\n",
    "    image_size_factor=1,\n",
    "    images_per_batch=1,\n",
    "    patches_per_batch=4,\n",
    ")\n",
    "\n",
    "image_list = [os.path.join(f\"{full_dataset.img_dir}\",f\"{name}.jpeg\") for name in params.validation_set.keys()]\n",
    "\n",
    "infer_dir = os.path.join(run_dir, \"infer\")\n",
    "os.makedirs(infer_dir, exist_ok=True)\n",
    "\n",
    "predictor.infer(\n",
    "    images=image_list,\n",
    "    output_dir=infer_dir,\n",
    "    nms_thresh=0.2,\n",
    "    score_thresh=0.8,\n",
    "    num_of_annotated_images_to_save=10,\n",
    "    save_bboxes=False,    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "https://github.com/maximusjv/AleketML/blob/main/aleket.ipynb",
     "timestamp": 1726685419266
    }
   ]
  },
  "kernelspec": {
   "display_name": ".wslvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
