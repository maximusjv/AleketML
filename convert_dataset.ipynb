{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO - DESCRIBE THIS FILE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install Pillow if not already installed\n",
    "%pip install pillow  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "from typing import Any, NewType\n",
    "import csv \n",
    "\n",
    "# Third-Party Libraries\n",
    "from PIL import Image, ImageDraw\n",
    "from PIL.TiffTags import TAGS\n",
    "from tqdm import tqdm\n",
    "\n",
    "from predictor import make_patches\n",
    "# Type Alias for Bounding Boxes\n",
    "Box = NewType('Box', tuple[int, int, int, int])\n",
    "\n",
    "# Dataset Paths (Remember to replace placeholders with your actual paths)\n",
    "DATASET_SOURCE = os.path.join( \"/mnt\", \"f\", \"dataUtils\", \"raw_data\")  # FIXME: Change accordingly\n",
    "OUTPUT_DATASET = os.path.join( \"/mnt\", \"e\", \"dataset\")   # FIXME: Change accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Math utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "def get_image_metadata(img: Image) -> dict[str, float]:\n",
    "    \"\"\"Extracts resolution metadata from a TIFF image.\n",
    "    Args:\n",
    "        img: A PIL Image object.\n",
    "    Returns:\n",
    "        A dictionary containing resolution information (x, y, and unit).\n",
    "    \"\"\"\n",
    "    tiff_tags = {TAGS.get(tag, tag): value for tag, value in img.tag.items()}\n",
    "        \n",
    "    res = {\n",
    "        \"x_resolution\": tiff_tags['XResolution'][0][0]/tiff_tags['XResolution'][0][1],\n",
    "        \"y_resolution\": tiff_tags['YResolution'][0][0]/tiff_tags['YResolution'][0][1],\n",
    "        \"resolution_unit\": tiff_tags['ResolutionUnit'] \n",
    "    }\n",
    "    return res\n",
    "\n",
    "class ImageUtils:\n",
    "    \"\"\"Provides utility functions for working with image resolutions and areas.\"\"\"\n",
    "\n",
    "    def __init__(self, img):\n",
    "        self.img = img\n",
    "        self.metadata = get_image_metadata(img)\n",
    "        self.x_resolution = float(self.metadata[\"x_resolution\"])\n",
    "        self.y_resolution = float(self.metadata[\"y_resolution\"])\n",
    "\n",
    "    def area_units_to_pixels(self, area):\n",
    "        \"\"\"Converts area from resolution units to pixels.\"\"\"\n",
    "        return float(area) * (max(self.x_resolution, self.y_resolution) ** 2)\n",
    "\n",
    "    def area_pixels_to_units(self, area):\n",
    "        \"\"\"Converts area from pixels to resolution units.\"\"\"\n",
    "        return float(area) / (max(self.x_resolution, self.y_resolution) ** 2)\n",
    "\n",
    "\n",
    "class ImageUtilOpener:\n",
    "    \"\"\"Context manager for opening and working with an image.\"\"\"\n",
    "\n",
    "    def __init__(self, file_name):\n",
    "        self.file_name = file_name\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.img = Image.open(self.file_name)\n",
    "        return ImageUtils(self.img)\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.img.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read csv+tif ImageJ-created dataset, and convert to dict\n",
    "of images_path as keys and corresponding bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cxcywh_to_xyxy(box: Box) -> Box:\n",
    "    \"\"\"Converts a bounding box from center-x, center-y, width, height format (CXCYWH) \n",
    "    to top-left-x, top-left-y, bottom-right-x, bottom-right-y format (XYXY).\n",
    "    Args:\n",
    "        box: A tuple representing the bounding box in CXCYWH format.\n",
    "    Returns:\n",
    "        A tuple representing the bounding box in XYXY format.\n",
    "    \"\"\"\n",
    "    cx, cy, w, h = box\n",
    "    xmin = cx-w//2\n",
    "    ymin = cy-h//2\n",
    "    xmax = cx+(w+1)//2  # ceil\n",
    "    ymax = cy+(h+1)//2  # ceil\n",
    "    return xmin, ymin, xmax, ymax\n",
    "\n",
    "def sanitize_annotations(name:str, annotation: dict[str, Any]) -> None:\n",
    "    \"\"\"Sanitizes annotations by removing duplicates and invalid bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        name: The name of the image associated with the annotations.\n",
    "        annotations: A dictionary containing \"category_id\" and \"boxes\" lists.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing sanitized \"category_id\" and \"boxes\" lists.\n",
    "    \"\"\"\n",
    "    sanitized_annotations = {\n",
    "        \"category_id\": [],\n",
    "        \"boxes\": []\n",
    "    }\n",
    "    \n",
    "    bbox_dict = dict()\n",
    "    for label, bbox in zip(annotation[\"category_id\"], annotation[\"boxes\"]):\n",
    "        if bbox in bbox_dict:\n",
    "            if bbox_dict[bbox] == label:\n",
    "                print(f\"WARNING: Duplicate bbox found in {name}: {bbox}\")\n",
    "            else:\n",
    "                print(f\"ERROR: Same bbox with different label found in {name}: {bbox}\")\n",
    "            continue\n",
    "        if any(coord < 0 for coord in bbox):\n",
    "            print(f\"Corrupted box found in {name}: {bbox}\")\n",
    "            continue\n",
    "\n",
    "        bbox_dict[bbox] = label\n",
    "\n",
    "    for key in bbox_dict:\n",
    "        sanitized_annotations[\"category_id\"].append(bbox_dict[key])\n",
    "        sanitized_annotations[\"boxes\"].append(key)\n",
    "\n",
    "    return sanitized_annotations\n",
    "        \n",
    "def convert_annotations(csv_path: str, img_path: str) -> dict[str, Any]:\n",
    "    \"\"\"Converts a CSV file and an image into annotations.\n",
    "    Args:\n",
    "        csv_path: The path to the CSV file containing annotations.\n",
    "        img_path: The path to the image file.\n",
    "    Returns:\n",
    "        A dictionary containing \"category_id\" and \"boxes\" lists representing the annotations.\n",
    "    \"\"\"\n",
    "    annotations = {}\n",
    "    annotations = {\n",
    "        \"category_id\": [],\n",
    "        \"boxes\": []\n",
    "    }\n",
    "    \n",
    "    with ImageUtilOpener(img_path) as util:\n",
    "        if os.path.exists(csv_path): \n",
    "            with open(csv_path) as data_file:\n",
    "                data = csv.reader(data_file)\n",
    "                next(data) # Skip header row\n",
    "                for row in data:\n",
    "                    try:\n",
    "                        _, label, area, category_id, _ = row  # Unpack row, ignore filename and category_name\n",
    "                        _, coordinates = label.split(':')  # Extract coordinates from label\n",
    "                        y, x = coordinates.split('-')\n",
    "                        y, x = int(y), int(x)\n",
    "\n",
    "                        area_in_pixels = util.area_units_to_pixels(float(area))\n",
    "                        bbox_side = int(math.sqrt(area_in_pixels / math.pi) * 2)  # Calculate square side length\n",
    "\n",
    "                        bbox = convert_cxcywh_to_xyxy((x, y, bbox_side, bbox_side))\n",
    "                        annotations[\"category_id\"].append(int(category_id))\n",
    "                        annotations[\"boxes\"].append(bbox)\n",
    "                    except Exception as e:\n",
    "                        \n",
    "                        annotations[\"boxes\"] = []\n",
    "                        annotations[\"category_id\"] = []\n",
    "                        print(f\"Error processing row in {csv_path}: {e}\")\n",
    "                        print(f\"{csv_path} Failed\")\n",
    "                        break  \n",
    "                     \n",
    "    return annotations\n",
    "    \n",
    "    \n",
    "def convert_dataset(data_path: str, imgs_path: str, res_path: str, replace_imgs=True):\n",
    "    \"\"\"Converts a dataset from the source format to the desired output format.\n",
    "    Args:\n",
    "        data_path: The path to the directory containing CSV annotation files\n",
    "        imgs_path: The path to the directory containing image files\n",
    "        res_path: The path to the output directory where the converted dataset will be saved\n",
    "        replace_imgs: Whether to replace existing images in the output directory\n",
    "    \"\"\"\n",
    "    img_list = [\n",
    "        os.path.splitext(f)[0] \n",
    "        for f in os.listdir(imgs_path) \n",
    "        if os.path.isfile(os.path.join(imgs_path, f)) and f.endswith(\".tif\")\n",
    "    ]\n",
    "    \n",
    "    if replace_imgs:\n",
    "        try:\n",
    "            shutil.rmtree(res_path)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    os.makedirs(os.path.join(res_path, \"imgs\"), exist_ok=True)\n",
    "       \n",
    "    dataset = {}\n",
    "    image_id = 1  # Start image IDs from 1\n",
    "    \n",
    "    for name in img_list:\n",
    "        csv_path = f'{data_path}/{name}.tif.csv'\n",
    "        img_path = f'{imgs_path}/{name}.tif'\n",
    "        annotations = convert_annotations(csv_path, img_path)\n",
    "        if not annotations[\"boxes\"]:\n",
    "            continue # Skip images without annotations\n",
    "\n",
    "        if replace_imgs:\n",
    "            img = Image.open(img_path)\n",
    "            img.save(os.path.join(res_path, \"imgs\", f\"{image_id}.jpeg\"), quality=100)\n",
    "        \n",
    "        annotations = sanitize_annotations(name, annotations)\n",
    "        dataset[str(image_id)] = annotations\n",
    "        \n",
    "        image_id += 1\n",
    "\n",
    "    with open(os.path.join(res_path, \"dataset.json\"), \"w\") as outfile:\n",
    "        json.dump(dataset, outfile, indent=1)\n",
    "    print(\"Finished converting\")\n",
    "    \n",
    "convert_dataset(f\"{DATASET_SOURCE}/csv\", f\"{DATASET_SOURCE}/imgs\", OUTPUT_DATASET, replace_imgs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patch dataset into images of desired size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def box_area(box: Box) -> float:\n",
    "    \"\"\"Calculates the box area\n",
    "    Args:\n",
    "        box: The box in XYXY format.\n",
    "\n",
    "    Returns:\n",
    "        The area of the box.\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = box\n",
    "    area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    return max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    \n",
    "\n",
    "def patch_annots(\n",
    "         cropped_img: Image.Image,\n",
    "         crop_box: Box,\n",
    "         annots: dict[str, Any],\n",
    "         crop_tolerance: float,\n",
    "         erase_cropped: bool = True) -> tuple[Image.Image, tuple[list[int], list[Box]]]:\n",
    "    \"\"\"Crops an image and adjusts annotations based oWSn the crop region.\n",
    "\n",
    "    Args:\n",
    "        cropped_img: The PIL Image object to crop\n",
    "        crop_box: The bounding box defining the crop region in XYXY format\n",
    "        annots: A dictionary containing \"category_id\" and \"boxes\" lists representing annotations\n",
    "        crop_tolerance: The tolerance for considering an annotation as fully within the crop\n",
    "        erase_cropped: Whether to draw over partially cropped annotations on the image\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the cropped image and the adjusted annotations\n",
    "    \"\"\"\n",
    "\n",
    "    cropped_annots = {\"category_id\": [], \"boxes\": []}\n",
    "    labels, bboxes = annots[\"category_id\"], annots[\"boxes\"]\n",
    "    \n",
    "    draw_context = ImageDraw.Draw(cropped_img)\n",
    "    for label, bbox in zip(labels, bboxes):\n",
    "        relative_bbox = (max(crop_box[0], bbox[0]) - crop_box[0], #xmin\n",
    "                         max(crop_box[1], bbox[1]) - crop_box[1], #ymin\n",
    "                         min(crop_box[2], bbox[2]) - crop_box[0], #xmax\n",
    "                         min(crop_box[3], bbox[3]) - crop_box[1]) #ymax\n",
    "        cropped = 1 - box_area(relative_bbox) / box_area(bbox)\n",
    "        if cropped <= crop_tolerance:\n",
    "            cropped_annots[\"category_id\"].append(label)\n",
    "            cropped_annots[\"boxes\"].append(relative_bbox)\n",
    "        elif cropped < 1 and erase_cropped:\n",
    "            draw_context.rectangle(relative_bbox, width=1, fill=\"black\")\n",
    "\n",
    "    return cropped_annots\n",
    "\n",
    "def patch_sample(img: Image.Image,\n",
    "                 annots: dict[str, Any],\n",
    "                 desired_image_size: int,\n",
    "                 overlap: float,\n",
    "                 crop_tolerance: float) -> tuple[list[Image.Image], list[tuple[list[int], list[Box]]]]:\n",
    "    \"\"\"Patches a large image into smaller images with adjusted annotations.\n",
    "    Args:\n",
    "        img: The PIL Image object to patch\n",
    "        annots: A tuple containing category_id and boxes lists representing annotations\n",
    "        desired_image_size: The desired size of each patch\n",
    "        overlap: The overlap between adjacent patches (as a fraction of `desired_image_size`)\n",
    "        crop_tolerance: The tolerance for considering an annotation as fully within a patch\n",
    "    Returns:\n",
    "        A tuple containing a list of patched images and a list of corresponding adjusted annotations\n",
    "    \"\"\"\n",
    "    \n",
    "    padded_width, padded_height, patch_boxes = make_patches(img.width, img.height, desired_image_size, overlap)\n",
    "    padded_img = Image.new(\"RGB\", (padded_width, padded_height))\n",
    "    padded_img.paste(img)\n",
    "    res_imgs, res_annots = [], []\n",
    "    \n",
    "    for patch_box in patch_boxes:\n",
    "        patched_img = padded_img.crop(patch_box)\n",
    "        patched_annots = patch_annots(patched_img, patch_box, annots, crop_tolerance)\n",
    "        res_imgs.append(patched_img)\n",
    "        res_annots.append(patched_annots)\n",
    "\n",
    "    return res_imgs, res_annots\n",
    "\n",
    "def patch_dataset(dataset_root: str,\n",
    "                  desired_image_size: int = 1024,\n",
    "                  overlap: float = 0.2,\n",
    "                  crop_tolerance: float=0.3):\n",
    "    \"\"\"Patches images in a dataset and saves the patched images and annotations.\n",
    "\n",
    "    Args:\n",
    "        dataset_root: The root directory of the dataset\n",
    "        desired_image_size: The desired size of each patch\n",
    "        overlap: The overlap between adjacent patches (as a fraction of `desired_image_size`)\n",
    "        crop_tolerance: The tolerance for considering an annotation as fully within a patch\n",
    "    \"\"\"\n",
    "    dataset_root = os.path.normpath(dataset_root)\n",
    "    annot_file = os.path.join(dataset_root, \"dataset.json\")\n",
    "    imgs_dir = os.path.join(dataset_root, \"imgs\")\n",
    "    dataset_parent = os.path.dirname(dataset_root)\n",
    "    patched_root = os.path.join(dataset_parent, os.path.basename(dataset_root) + \"_patched_croptolerance=\" + str(crop_tolerance))\n",
    "    os.makedirs(os.path.join(patched_root, \"imgs\"), exist_ok=True)\n",
    "    \n",
    "    patched_dataset = {}\n",
    "    \n",
    "    with open(annot_file, 'r') as annot_file:\n",
    "        dataset = json.load(annot_file)\n",
    "        \n",
    "    for image_id, annotations in tqdm(dataset.items()):\n",
    "        \n",
    "        img = Image.open(os.path.join(imgs_dir, f\"{image_id}.jpeg\"))\n",
    "        patched_imgs, patched_annots = patch_sample(img, annotations, desired_image_size, overlap, crop_tolerance)\n",
    "        patch_num = 1\n",
    "        \n",
    "        for patched_image, patched_annotations in zip(patched_imgs, patched_annots):\n",
    "            patch_id = f\"{image_id}_{patch_num}\"\n",
    "            patched_dataset[patch_id] = patched_annotations\n",
    "            patched_image.save(os.path.join(patched_root, \"imgs\", patch_id + \".jpeg\"), quality=95)\n",
    "            patch_num += 1\n",
    "            \n",
    "    with open(os.path.join(patched_root, \"dataset.json\"), \"w\") as outfile: \n",
    "        json.dump(patched_dataset, outfile, indent=1)\n",
    "        \n",
    "patch_dataset(OUTPUT_DATASET, crop_tolerance=0.7) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".wslvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
