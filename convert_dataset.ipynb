{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO - DESCRIBE THIS FILE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from PIL.TiffTags import TAGS\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.patches import make_patches\n",
    "\n",
    "# Dataset Paths (Remember to replace placeholders with your actual paths)\n",
    "DATASET_SOURCE = os.path.join(\n",
    "    \"/mnt\", \"f\", \"dataUtils\", \"raw_data\"\n",
    ")  # FIXME: Change accordingly\n",
    "OUTPUT_DATASET = os.path.join(\"/mnt\", \"e\", \"dataset\")  # FIXME: Change accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cxcywh_to_xyxy(box):\n",
    "    \"\"\"Converts a bounding box from center-x, center-y, width, height format (CXCYWH)\n",
    "    to top-left-x, top-left-y, bottom-right-x, bottom-right-y format (XYXY).\n",
    "\n",
    "    Args:\n",
    "        box: A tuple representing the bounding box in CXCYWH format.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple representing the bounding box in XYXY format.\n",
    "    \"\"\"\n",
    "    cx, cy, w, h = box\n",
    "    xmin = cx - w // 2\n",
    "    ymin = cy - h // 2\n",
    "    xmax = cx + (w + 1) // 2  # ceil\n",
    "    ymax = cy + (h + 1) // 2  # ceil\n",
    "    return xmin, ymin, xmax, ymax\n",
    "\n",
    "\n",
    "def get_image_metadata(img):\n",
    "    \"\"\"Extracts resolution metadata from a TIFF image.\n",
    "\n",
    "    Args:\n",
    "        img: A PIL Image object.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, float]: A dictionary containing resolution information (x, y, and unit).\n",
    "    \"\"\"\n",
    "    tiff_tags = {TAGS.get(tag, tag): value for tag, value in img.tag.items()}\n",
    "\n",
    "    res = {\n",
    "        \"x_resolution\": tiff_tags[\"XResolution\"][0][0] / tiff_tags[\"XResolution\"][0][1],\n",
    "        \"y_resolution\": tiff_tags[\"YResolution\"][0][0] / tiff_tags[\"YResolution\"][0][1],\n",
    "        \"resolution_unit\": tiff_tags[\"ResolutionUnit\"],\n",
    "    }\n",
    "    return res\n",
    "\n",
    "\n",
    "class ImageUtils:\n",
    "    \"\"\"Provides utility functions for working with image resolutions and areas.\"\"\"\n",
    "\n",
    "    def __init__(self, img):\n",
    "        self.img = img\n",
    "        self.metadata = get_image_metadata(img)\n",
    "        self.x_resolution = float(self.metadata[\"x_resolution\"])\n",
    "        self.y_resolution = float(self.metadata[\"y_resolution\"])\n",
    "\n",
    "    def area_units_to_pixels(self, area):\n",
    "        \"\"\"Converts area from resolution units to pixels.\"\"\"\n",
    "        return float(area) * (max(self.x_resolution, self.y_resolution) ** 2)\n",
    "\n",
    "    def area_pixels_to_units(self, area):\n",
    "        \"\"\"Converts area from pixels to resolution units.\"\"\"\n",
    "        return float(area) / (max(self.x_resolution, self.y_resolution) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_annotations(name, annotation) -> None:\n",
    "    \"\"\"Sanitizes annotations by removing duplicates and invalid bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        name: The name of the image associated with the annotations.\n",
    "        annotations: A dictionary containing \"category_id\" and \"boxes\" lists.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing sanitized \"category_id\" and \"boxes\" lists.\n",
    "    \"\"\"\n",
    "    sanitized_annotations = {\"category_id\": [], \"boxes\": []}\n",
    "\n",
    "    bbox_dict = dict()\n",
    "    for label, bbox in zip(annotation[\"category_id\"], annotation[\"boxes\"]):\n",
    "        if bbox in bbox_dict:\n",
    "            if bbox_dict[bbox] == label:\n",
    "                print(f\"WARNING: Duplicate bbox found in {name}: {bbox}\")\n",
    "            else:\n",
    "                print(f\"ERROR: Same bbox with different label found in {name}: {bbox}\")\n",
    "            continue\n",
    "        if any(coord < 0 for coord in bbox):\n",
    "            print(f\"Corrupted box found in {name}: {bbox}\")\n",
    "            continue\n",
    "\n",
    "        bbox_dict[bbox] = label\n",
    "\n",
    "    for key in bbox_dict:\n",
    "        sanitized_annotations[\"category_id\"].append(bbox_dict[key])\n",
    "        sanitized_annotations[\"boxes\"].append(key)\n",
    "\n",
    "    return sanitized_annotations\n",
    "\n",
    "\n",
    "def convert_annotations(csv_path, img_path):\n",
    "    \"\"\"Converts a CSV file and an image into annotations.\n",
    "    Args:\n",
    "        csv_path: The path to the CSV file containing annotations.\n",
    "        img_path: The path to the image file.\n",
    "    Returns:\n",
    "        A dictionary containing \"category_id\" and \"boxes\" lists representing the annotations.\n",
    "    \"\"\"\n",
    "    annotations = {\"category_id\": [], \"boxes\": []}\n",
    "\n",
    "    util = ImageUtils(Image.open(img_path))\n",
    "    if os.path.exists(csv_path):\n",
    "        with open(csv_path) as data_file:\n",
    "            data = csv.reader(data_file)\n",
    "            next(data)  # Skip header row\n",
    "            for row in data:\n",
    "                try:\n",
    "                    _, label, area, category_id, _ = (\n",
    "                        row  # Unpack row, ignore filename and category_name\n",
    "                    )\n",
    "                    _, coordinates = label.split(\":\")  # Extract coordinates from label\n",
    "                    y, x = coordinates.split(\"-\")\n",
    "                    y, x = int(y), int(x)\n",
    "\n",
    "                    area_in_pixels = util.area_units_to_pixels(float(area))\n",
    "                    bbox_side = int(\n",
    "                        math.sqrt(area_in_pixels / math.pi) * 2\n",
    "                    )  # Calculate square side length\n",
    "\n",
    "                    bbox = convert_cxcywh_to_xyxy((x, y, bbox_side, bbox_side))\n",
    "                    annotations[\"category_id\"].append(int(category_id))\n",
    "                    annotations[\"boxes\"].append(bbox)\n",
    "                except Exception as e:\n",
    "                    annotations[\"boxes\"] = []\n",
    "                    annotations[\"category_id\"] = []\n",
    "\n",
    "                    print(f\"Error processing row in {csv_path}: {e}\")\n",
    "                    print(f\"{csv_path} Failed\")\n",
    "                    break\n",
    "\n",
    "    return annotations\n",
    "\n",
    "\n",
    "def convert_dataset(data_path, imgs_path, res_path, replace_imgs=True):\n",
    "    \"\"\"Converts a dataset from the source format to the desired output format.\n",
    "    Args:\n",
    "        data_path: The path to the directory containing CSV annotation files\n",
    "        imgs_path: The path to the directory containing image files\n",
    "        res_path: The path to the output directory where the converted dataset will be saved\n",
    "        replace_imgs: Whether to replace existing images in the output directory\n",
    "    \"\"\"\n",
    "    img_list = [\n",
    "        os.path.splitext(f)[0]\n",
    "        for f in os.listdir(imgs_path)\n",
    "        if os.path.isfile(os.path.join(imgs_path, f)) and f.endswith(\".tif\")\n",
    "    ]\n",
    "\n",
    "    if replace_imgs:\n",
    "        try:\n",
    "            shutil.rmtree(res_path)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    os.makedirs(os.path.join(res_path, \"imgs\"), exist_ok=True)\n",
    "\n",
    "    dataset = {}\n",
    "    image_id = 1  # Start image IDs from 1\n",
    "\n",
    "    # Prepare to write stats to CSV\n",
    "    stats_data = []\n",
    "\n",
    "    for name in img_list:\n",
    "        csv_path = f\"{data_path}/{name}.tif.csv\"\n",
    "        img_path = f\"{imgs_path}/{name}.tif\"\n",
    "        annotations = convert_annotations(csv_path, img_path)\n",
    "        if not annotations[\"boxes\"]:\n",
    "            continue  # Skip images without annotations\n",
    "\n",
    "        if replace_imgs:\n",
    "            img = Image.open(img_path)\n",
    "            img.save(os.path.join(res_path, \"imgs\", f\"{image_id}.jpeg\"), quality=100)\n",
    "\n",
    "        annotations = sanitize_annotations(name, annotations)\n",
    "        dataset[str(image_id)] = annotations\n",
    "\n",
    "        # Calculate and store stats\n",
    "        healthy_area = 0\n",
    "        necrosed_area = 0\n",
    "        healthy_count = 0\n",
    "        necrosed_count = 0\n",
    "        for bbox, class_id in zip(annotations[\"boxes\"], annotations[\"category_id\"]):\n",
    "            if class_id == 1:  # Healthy area\n",
    "                healthy_area += math.pi * ((bbox[2] - bbox[0]) / 2) ** 2\n",
    "                healthy_count += 1\n",
    "            else:  # Necrosed area\n",
    "                necrosed_area += math.pi * ((bbox[2] - bbox[0]) / 2) ** 2\n",
    "                necrosed_count += 1\n",
    "\n",
    "        stats_data.append(\n",
    "            [\n",
    "                f\"{image_id}.jpeg\",\n",
    "                int(healthy_area),\n",
    "                int(necrosed_area),\n",
    "                healthy_count,\n",
    "                necrosed_count,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        image_id += 1\n",
    "\n",
    "    with open(os.path.join(res_path, \"dataset.json\"), \"w\") as outfile:\n",
    "        json.dump(dataset, outfile, indent=1)\n",
    "\n",
    "    # Write stats to CSV\n",
    "    with open(os.path.join(res_path, \"stats.csv\"), \"w\", newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(\n",
    "            [\n",
    "                \"Image\",\n",
    "                \"healthy area\",\n",
    "                \"necrosed area\",\n",
    "                \"healthy count\",\n",
    "                \"necrosed count\",\n",
    "            ]\n",
    "        )  # Write header\n",
    "        writer.writerows(stats_data)\n",
    "\n",
    "    print(\"Finished converting\")\n",
    "\n",
    "\n",
    "convert_dataset(\n",
    "    f\"{DATASET_SOURCE}/csv\",\n",
    "    f\"{DATASET_SOURCE}/imgs\",\n",
    "    OUTPUT_DATASET,\n",
    "    replace_imgs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_area(box):\n",
    "    \"\"\"Calculates the box area\n",
    "\n",
    "    Args:\n",
    "        box: The box in XYXY format.\n",
    "\n",
    "    Returns:\n",
    "        float: The area of the box.\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = box\n",
    "    return max(0, x2 - x1) * max(0, y2 - y1)\n",
    "\n",
    "\n",
    "def patch_annots(cropped_img, crop_box, annots, crop_tolerance, erase_cropped=True):\n",
    "    \"\"\"\n",
    "    Adjusts annotations for a cropped image patch.\n",
    "\n",
    "    This function takes annotations for an image and adjusts them to match a cropped region.\n",
    "    It filters out annotations that are mostly outside the cropped region and optionally erases\n",
    "    partially cropped bounding boxes from the image.\n",
    "\n",
    "    Args:\n",
    "        cropped_img (PIL.Image.Image): The cropped image patch.\n",
    "        crop_box (tuple): The coordinates (xmin, ymin, xmax, ymax) of the crop box in the original image.\n",
    "        annots (dict): A dictionary containing the annotations with keys 'category_id' and 'boxes'.\n",
    "        crop_tolerance (float): The maximum allowed fraction of a bounding box that can be cropped \n",
    "                                 before it is removed.\n",
    "        erase_cropped (bool, optional): Whether to erase partially cropped bounding boxes from the image. \n",
    "                                        Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the adjusted annotations for the cropped image with keys \n",
    "              'category_id' and 'boxes'.\n",
    "    \"\"\"\n",
    "\n",
    "    cropped_annots = {\"category_id\": [], \"boxes\": []}  # Initialize a dictionary to store the cropped annotations\n",
    "    labels, bboxes = annots[\"category_id\"], annots[\"boxes\"]  # Extract labels and bounding boxes from the input annotations\n",
    "\n",
    "    draw_context = ImageDraw.Draw(cropped_img)  # Create a drawing context for the cropped image\n",
    "    for label, bbox in zip(labels, bboxes):  # Iterate over each label and bounding box\n",
    "        # Calculate the coordinates of the bounding box relative to the cropped image\n",
    "        relative_bbox = (\n",
    "            max(crop_box[0], bbox[0]) - crop_box[0],  # xmin\n",
    "            max(crop_box[1], bbox[1]) - crop_box[1],  # ymin\n",
    "            min(crop_box[2], bbox[2]) - crop_box[0],  # xmax\n",
    "            min(crop_box[3], bbox[3]) - crop_box[1],  # ymax\n",
    "        )\n",
    "        cropped = 1 - box_area(relative_bbox) / box_area(bbox)  # Calculate the fraction of the bounding box that is cropped\n",
    "        if cropped <= crop_tolerance:  # If the cropped fraction is within the tolerance\n",
    "            cropped_annots[\"category_id\"].append(label)  # Keep the annotation\n",
    "            cropped_annots[\"boxes\"].append(relative_bbox)  # Add the relative bounding box to the cropped annotations\n",
    "        elif cropped < 1 and erase_cropped:  # If the bounding box is partially cropped and erase_cropped is True\n",
    "            draw_context.rectangle(relative_bbox, width=1, fill=\"black\")  # Erase the cropped part of the bounding box\n",
    "\n",
    "    return cropped_annots  # Return the adjusted annotations for the cropped image\n",
    "\n",
    "\n",
    "def patch_sample(img, annots, desired_image_size, overlap, crop_tolerance):\n",
    "    \"\"\"\n",
    "    Splits an image and its annotations into overlapping patches.\n",
    "\n",
    "    Args:\n",
    "        img (PIL.Image.Image): The input image to be patched.\n",
    "        annots (dict): A dictionary containing the annotations for the image with keys 'category_id' and 'boxes'.\n",
    "        desired_image_size (int): The desired size of each patch.\n",
    "        overlap (float): The fraction of overlap between patches.\n",
    "        crop_tolerance (float): The maximum allowed fraction of a bounding box that can be cropped \n",
    "                                 before it is removed.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two lists:\n",
    "            - res_imgs: A list of PIL Image objects representing the image patches.\n",
    "            - res_annots: A list of dictionaries, each containing the adjusted annotations for the corresponding patch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate patch coordinates and padded image dimensions\n",
    "    padded_width, padded_height, patch_boxes = make_patches(\n",
    "        img.width, img.height, desired_image_size, overlap\n",
    "    )\n",
    "    padded_img = Image.new(\"RGB\", (padded_width, padded_height))  # Create a new image with padding\n",
    "    padded_img.paste(img)  # Paste the original image onto the padded image\n",
    "    res_imgs, res_annots = [], []  # Initialize lists to store the resulting patches and annotations\n",
    "\n",
    "    for patch_box in patch_boxes:  # Iterate over each patch box\n",
    "        patched_img = padded_img.crop(patch_box)  # Crop the padded image to create a patch\n",
    "        # Adjust the annotations for the cropped patch\n",
    "        patched_annots_ = patch_annots(patched_img, patch_box, annots, crop_tolerance)  \n",
    "        res_imgs.append(patched_img)  # Add the patch to the list of results\n",
    "        res_annots.append(patched_annots_)  # Add the adjusted annotations to the list of results\n",
    "\n",
    "    return res_imgs, res_annots  # Return the lists of patches and corresponding annotations\n",
    "\n",
    "def patch_dataset(\n",
    "    dataset_root, desired_image_size=1024, overlap=0.2, crop_tolerance=0.3\n",
    "):\n",
    "    \"\"\"Patches images in a dataset and saves the patched images and annotations.\n",
    "\n",
    "    Args:\n",
    "        dataset_root (str): The root directory of the dataset\n",
    "        desired_image_size (int, optional): The desired size of each patch. Defaults to 1024.\n",
    "        overlap (float, optional): The overlap between adjacent patches (as a fraction of\n",
    "                                    `desired_image_size`). Defaults to 0.2.\n",
    "        crop_tolerance (float, optional): The tolerance for considering an annotation as\n",
    "                                        fully within a patch. Defaults to 0.3.\n",
    "    \"\"\"\n",
    "    dataset_root = os.path.normpath(dataset_root)\n",
    "    annot_file = os.path.join(dataset_root, \"dataset.json\")\n",
    "    imgs_dir = os.path.join(dataset_root, \"imgs\")\n",
    "    dataset_parent = os.path.dirname(dataset_root)\n",
    "    patched_root = os.path.join(\n",
    "        dataset_parent,\n",
    "        os.path.basename(dataset_root)\n",
    "        + \"_patched_croptolerance=\"\n",
    "        + str(crop_tolerance),\n",
    "    )\n",
    "    os.makedirs(os.path.join(patched_root, \"imgs\"), exist_ok=True)\n",
    "\n",
    "    patched_dataset = {}\n",
    "\n",
    "    with open(annot_file, \"r\") as annot_file:\n",
    "        dataset = json.load(annot_file)\n",
    "\n",
    "    for image_id, annotations in tqdm(dataset.items()):\n",
    "\n",
    "        img = Image.open(os.path.join(imgs_dir, f\"{image_id}.jpeg\"))\n",
    "        patched_imgs, patched_annots = patch_sample(\n",
    "            img, annotations, desired_image_size, overlap, crop_tolerance\n",
    "        )\n",
    "        patch_num = 1\n",
    "\n",
    "        for patched_image, patched_annotations in zip(patched_imgs, patched_annots):\n",
    "            patch_id = f\"{image_id}_{patch_num}\"\n",
    "            patched_dataset[patch_id] = patched_annotations\n",
    "            patched_image.save(\n",
    "                os.path.join(patched_root, \"imgs\", patch_id + \".jpeg\"), quality=95\n",
    "            )\n",
    "            patch_num += 1\n",
    "\n",
    "    with open(os.path.join(patched_root, \"dataset.json\"), \"w\") as outfile:\n",
    "        json.dump(patched_dataset, outfile, indent=1)\n",
    "\n",
    "\n",
    "patch_dataset(OUTPUT_DATASET, crop_tolerance=0.7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".wslvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
