{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKXbvPtmIAzW"
   },
   "source": [
    "# Aleket Faster R-CNN training notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10801,
     "status": "ok",
     "timestamp": 1726688877827,
     "user": {
      "displayName": "maximus JV",
      "userId": "01037924598235015782"
     },
     "user_tz": -180
    },
    "id": "oLaoTSNaIAzX"
   },
   "outputs": [],
   "source": [
    "%pip install pillow\n",
    "%pip install numpy<2.0\n",
    "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cu124\n",
    "%pip install matplotlib\n",
    "%pip install pycocotools\n",
    "%pip install gdown\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "print(\"ALL DEPENDENCIES INSTALLED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 7480,
     "status": "ok",
     "timestamp": 1726688885304,
     "user": {
      "displayName": "maximus JV",
      "userId": "01037924598235015782"
     },
     "user_tz": -180
    },
    "id": "QYh3of1UgFZs"
   },
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "# Standard Library\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Third-Party Libraries\n",
    "import numpy as np\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "\n",
    "# Torchvision\n",
    "import torchvision.models.detection as tv_detection\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "import torchvision.transforms.v2 as v2\n",
    "\n",
    "# Utils\n",
    "from utils import AleketDataset, TrainingLogger, StatsTracker, download_dataset, split_dataset\n",
    "from training_and_evaluation import evaluate, train_one_epoch, CocoEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "# Device Selection\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Model Builder\n",
    "def get_model(num_classes) -> FasterRCNN:\n",
    "    \"\"\"Loads or creates a Faster R-CNN model.\n",
    "    Args:\n",
    "        num_classes: The number of classes in the dataset.\n",
    "\n",
    "    Returns:\n",
    "        The Faster R-CNN model on the specified device.\n",
    "    \"\"\"\n",
    "    model = tv_detection.fasterrcnn_resnet50_fpn(\n",
    "        weights=\"DEFAULT\"\n",
    "    )\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = (\n",
    "        tv_detection.faster_rcnn.FastRCNNPredictor(\n",
    "            in_features, num_classes\n",
    "        )\n",
    "    )\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "# Save training state\n",
    "def save_checkpoint(model: FasterRCNN,\n",
    "                    optimizer: SGD,\n",
    "                    lr_scheduler: LinearLR,\n",
    "                    epoch_trained: int,\n",
    "                    checkpoint_path: str) -> None:\n",
    "    save_state = {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"lr_scheduler_state_dict\": lr_scheduler.state_dict(),\n",
    "        \"epoch_trained\": epoch_trained, \n",
    "    }\n",
    "    torch.save(save_state, checkpoint_path)\n",
    "\n",
    "# Load training state\n",
    "def load_checkpoint(\n",
    "    checkpoint_path: str) -> tuple[FasterRCNN, SGD, LinearLR]:\n",
    "    \n",
    "        save_state = torch.load(checkpoint_path, weights_only=False)\n",
    "        \n",
    "        model = get_model(3)\n",
    "        model.load_state_dict(save_state[\"model_state_dict\"])\n",
    "        optimizer = SGD(model.parameters())\n",
    "        optimizer.load_state_dict(save_state[\"optimizer_state_dict\"])\n",
    "        \n",
    "        epoch_trained = save_state[\"epoch_trained\"]\n",
    "               \n",
    "        lr_scheduler = LinearLR(optimizer, last_epoch=epoch_trained)\n",
    "        lr_scheduler.load_state_dict(save_state[\"lr_scheduler_state_dict\"])\n",
    "        \n",
    "        \n",
    "        return model, optimizer, lr_scheduler, epoch_trained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G9UdbXiJIAzZ",
    "outputId": "4eaac050-8192-4ffe-e5ae-ee59701ce7b2"
   },
   "outputs": [],
   "source": [
    "# Random Seed for Reproducibility\n",
    "SEED = 1\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Path variables\n",
    "RESUME = False\n",
    "RESULT_PATH = \"result\"\n",
    "DATASET_ROOT = download_dataset(\"dataset_patched\")\n",
    "\n",
    "os.makedirs(os.path.join(RESULT_PATH, \"run\"), exist_ok=True)\n",
    "LAST_CHECKPOINT = os.path.join(RESULT_PATH, \"run\", \"last.pth\")\n",
    "BEST_CHECKPOINT = os.path.join(RESULT_PATH, \"run\", \"best.pth\")   \n",
    "STATS_GRAPH = os.path.join(RESULT_PATH, \"stats_graph\")   \n",
    "STATS_LOG = os.path.join(RESULT_PATH, \"stats.csv\")   \n",
    "\n",
    "# Dataset split\n",
    "TEST_FRACTION = 0.2\n",
    "DATASET_FRACTION = 0.01\n",
    "DATALOADER_WORKERS = 16\n",
    "\n",
    "# Training Hyperparameters\n",
    "IMG_SIZE = 1024\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "LR = 0.01\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.0001\n",
    "WARMUP_EPOCHS = 50\n",
    "\n",
    "# Data Augmentation Transforms\n",
    "\n",
    "DEFAULT_TRANSFORMS = v2.Compose(\n",
    "    [v2.Resize(size=IMG_SIZE), v2.ToDtype(torch.float32, scale=True), ]\n",
    ")\n",
    "   \n",
    "TRAINING_TRANSFORMS = v2.Compose(\n",
    "    [\n",
    "        v2.RandomHorizontalFlip(p=0.5),\n",
    "        v2.RandomVerticalFlip(p=0.5),\n",
    "        v2.RandomPerspective(distortion_scale=0.2, p=0.2),\n",
    "        v2.RandomRotation(degrees=(-10, 10), expand=True),\n",
    "        DEFAULT_TRANSFORMS,\n",
    "    ]\n",
    ")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training and evaluation loop.\"\"\"\n",
    "    \n",
    "    dataset = AleketDataset(DATASET_ROOT, DEFAULT_TRANSFORMS)\n",
    "    # Calculate the number of samples to use based on the dataset_fraction\n",
    "    num_samples = int(len(dataset) * DATASET_FRACTION)\n",
    "    indices = torch.randperm(len(dataset)).tolist()[:num_samples]\n",
    "    test_size = int(num_samples * TEST_FRACTION)\n",
    "    \n",
    "    train_indicies, val_indicies = indices[:-test_size], indices[-test_size:]\n",
    "    \n",
    "    train_dataloader, val_dataloader = split_dataset(\n",
    "            dataset, train_indicies, val_indicies, BATCH_SIZE, DATALOADER_WORKERS\n",
    "    ) \n",
    "           \n",
    "    coco_eval = CocoEvaluator(val_dataloader.dataset)       \n",
    "           \n",
    "    model = get_model(3)\n",
    "    optimizer = SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "    lr_scheduler = LinearLR(\n",
    "        optimizer, start_factor=1, end_factor=0.1, total_iters=WARMUP_EPOCHS\n",
    "    )\n",
    "    \n",
    "    epoch_trained = 0\n",
    "    stats_tracker = StatsTracker()\n",
    "    logger = TrainingLogger(STATS_LOG)\n",
    "    \n",
    "    if RESUME:\n",
    "        print(f\"Resuming from  {LAST_CHECKPOINT}...\")\n",
    "        model, optimizer, lr_scheduler, epoch_trained = load_checkpoint(LAST_CHECKPOINT)\n",
    "    \n",
    "    while epoch_trained < EPOCHS:\n",
    "        \n",
    "        epoch = epoch_trained+1\n",
    "        logger.log_epoch_start(epoch, EPOCHS, lr_scheduler.get_last_lr()[0])\n",
    "        \n",
    "        dataset.transforms = TRAINING_TRANSFORMS\n",
    "        losses = train_one_epoch(\n",
    "            model, optimizer, train_dataloader, DEVICE\n",
    "        )\n",
    "        \n",
    "        dataset.transforms = DEFAULT_TRANSFORMS\n",
    "        eval_stats = evaluate(\n",
    "            model, val_dataloader, coco_eval, DEVICE\n",
    "        )\n",
    "        \n",
    "        stats_tracker.update_train_loss(losses)\n",
    "        is_best = stats_tracker.update_val_metrics(eval_stats)\n",
    "\n",
    "        logger.log_epoch_end(epoch, losses, eval_stats)\n",
    "        stats_tracker.plot_stats(STATS_GRAPH)\n",
    "        \n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        epoch_trained = epoch\n",
    "        \n",
    "        save_checkpoint(model,optimizer, lr_scheduler, epoch_trained, LAST_CHECKPOINT)\n",
    "        if is_best:\n",
    "            save_checkpoint(model,optimizer, lr_scheduler, epoch_trained, BEST_CHECKPOINT)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "https://github.com/maximusjv/AleketML/blob/main/aleket.ipynb",
     "timestamp": 1726685419266
    }
   ]
  },
  "kernelspec": {
   "display_name": ".wslvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
