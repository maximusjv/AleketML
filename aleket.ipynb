{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKXbvPtmIAzW"
   },
   "source": [
    "# Aleket Faster R-CNN training notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10801,
     "status": "ok",
     "timestamp": 1726688877827,
     "user": {
      "displayName": "maximus JV",
      "userId": "01037924598235015782"
     },
     "user_tz": -180
    },
    "id": "oLaoTSNaIAzX"
   },
   "outputs": [],
   "source": [
    "%pip install pillow\n",
    "%pip install numpy<2.0\n",
    "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cu124\n",
    "%pip install matplotlib\n",
    "%pip install pycocotools\n",
    "%pip install gdown\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "print(\"ALL DEPENDENCIES INSTALLED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 7480,
     "status": "ok",
     "timestamp": 1726688885304,
     "user": {
      "displayName": "maximus JV",
      "userId": "01037924598235015782"
     },
     "user_tz": -180
    },
    "id": "QYh3of1UgFZs"
   },
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "# Standard Library\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from typing import Optional\n",
    "\n",
    "# Third-Party Libraries\n",
    "import gdown\n",
    "import numpy as np\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# Torchvision\n",
    "import torchvision.models.detection as tv_detection\n",
    "import torchvision.transforms.v2 as v2\n",
    "\n",
    "# Utils\n",
    "from utils import AleketDataset, TrainingLogger, StatsTracker\n",
    "from training_and_evaluation import evaluate, train_one_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "\n",
    "# Device Selection\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Dataset split\n",
    "def split_dataset(\n",
    "    dataset: AleketDataset,\n",
    "    train_indicies: list[int],\n",
    "    val_indicies: list[int],\n",
    "    batch_size: int,\n",
    "    num_workers: int,\n",
    ") -> tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"Divides the dataset into training and validation sets and creates DataLoaders.\n",
    "    Args:\n",
    "        dataset: The AleketDataset to divide.\n",
    "        train_indicies: Dataset indicies to train\n",
    "        test_fraction: The fraction of the used dataset to allocate for validation.\n",
    "        batch_size: The batch size for the DataLoaders.\n",
    "        num_workers: The number of worker processes for data loading.\n",
    "    Returns:\n",
    "        A tuple containing the training DataLoader and the validation DataLoader.\n",
    "    \"\"\"\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        \"\"\"Collates data samples into batches for the dataloader.\"\"\"\n",
    "        return tuple(zip(*batch))\n",
    "\n",
    "    # Create training and validation subsets\n",
    "    train_dataset = Subset(dataset, train_indicies)\n",
    "    val_dataset = Subset(dataset, val_indicies)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "def get_model(num_classes) -> tv_detection.FasterRCNN:\n",
    "    \"\"\"Loads or creates a Faster R-CNN model.\n",
    "    Args:\n",
    "        num_classes: The number of classes in the dataset.\n",
    "\n",
    "    Returns:\n",
    "        The Faster R-CNN model on the specified device.\n",
    "    \"\"\"\n",
    "    model = tv_detection.fasterrcnn_resnet50_fpn(\n",
    "        weights=\"DEFAULT\"\n",
    "    )\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = (\n",
    "        tv_detection.faster_rcnn.FastRCNNPredictor(\n",
    "            in_features, num_classes\n",
    "        )\n",
    "    )\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "class Trainer: \n",
    "    \"\"\"\n",
    "    A class to manage the training and evaluation of a FasterRCNN model.\n",
    "\n",
    "    Args:\n",
    "        dataset (AleketDataset): The dataset to use for training and evaluation.\n",
    "        train_indicies (list[int]): List of indices for the training set.\n",
    "        val_inidices (list[int]): List of indices for the validation set.\n",
    "        batch_size (int): The batch size to use for training and evaluation.\n",
    "        num_workers (int): The number of worker processes to use for data loading.\n",
    "        model (FasterRCNN): The FasterRCNN model to train.\n",
    "        optimizer (optim.optimizer.Optimizer): The optimizer to use for training.\n",
    "        lr_scheduler (optim.lr_scheduler.LRScheduler): The learning rate scheduler to use for training.\n",
    "        result_path (str): The path to save training results and checkpoints.\n",
    "        total_epochs (int): The total number of epochs to train for.\n",
    "        last_epoch (int, optional): The last epoch completed. Defaults to 0.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 dataset: AleketDataset, \n",
    "                 train_indicies: list[int],\n",
    "                 val_inidices: list[int],\n",
    "                 batch_size: int,\n",
    "                 num_workers: int,\n",
    "                 model: tv_detection.FasterRCNN,\n",
    "                 optimizer: optim.Optimizer,\n",
    "                 lr_scheduler: optim.lr_scheduler.LRScheduler,\n",
    "                 result_path: str,\n",
    "                 total_epochs: int,\n",
    "                 stats_tracker: StatsTracker,\n",
    "                 last_epoch: int = 0\n",
    "                 ) -> None:\n",
    "        \n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.train_indicies = train_indicies\n",
    "        self.val_indicies = val_inidices\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        self.train_dataloader, self.val_dataloader = split_dataset(\n",
    "            dataset, train_indicies, val_inidices, batch_size, num_workers\n",
    "            )\n",
    "\n",
    "        self.result_path = result_path\n",
    "        os.makedirs(os.path.join(result_path, \"run\"), exist_ok=True)\n",
    "        self.last = os.path.join(result_path, \"run\", \"last.pth\")\n",
    "        self.best = os.path.join(result_path, \"run\", \"best.pth\")   \n",
    "        self.stats_graph = os.path.join(result_path, \"stats_graph\")\n",
    "        log_file = os.path.join(result_path, \"stats.csv\")\n",
    "        self.logger = TrainingLogger(log_file)\n",
    "        \n",
    "        self.stats_tracker = stats_tracker\n",
    "        \n",
    "        self.model = model\n",
    "        self.optimizer = optimizer \n",
    "        self.lr_scheduler = lr_scheduler \n",
    "        \n",
    "        self.last_epoch = last_epoch\n",
    "        self.total_epochs = total_epochs\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the FasterRCNN model for the specified number of epochs.\n",
    "        \"\"\"\n",
    "        while self.last_epoch < self.total_epochs:\n",
    "            epoch = self.last_epoch+1\n",
    "            self.logger.log_epoch_start(epoch, self.total_epochs, self.lr_scheduler.get_last_lr()[0])\n",
    "            \n",
    "            self.dataset.train = True\n",
    "            losses = train_one_epoch(\n",
    "                self.model, self.optimizer, self.train_dataloader, DEVICE\n",
    "            )\n",
    "\n",
    "            self.dataset.train = False\n",
    "            eval_stats = evaluate(\n",
    "                self.model, self.val_dataloader, DEVICE\n",
    "            )\n",
    "            \n",
    "            self.stats_tracker.update_train_loss(losses)\n",
    "            is_best = self.stats_tracker.update_val_metrics(eval_stats)\n",
    "\n",
    "            self.logger.log_epoch_end(epoch, losses,eval_stats)\n",
    "            \n",
    "            self.save_state(is_best)\n",
    "            \n",
    "            self.lr_scheduler.step()\n",
    "            self.stats_tracker.plot_stats(\"training_stats\")\n",
    "            \n",
    "            self.last_epoch = epoch  # Update last_epoch after each epoch\n",
    "\n",
    "        \n",
    "    def save_state(self, is_best: bool):\n",
    "        \"\"\"\n",
    "        Saves the current training state to a file.\n",
    "\n",
    "        Args:\n",
    "            is_best (bool): Whether the current state is the best so far.\n",
    "        \"\"\"\n",
    "        save_state = {\n",
    "            \"dataset\": self.dataset,\n",
    "            \"train_indicies\": self.train_indicies,\n",
    "            \"val_indicies\": self.val_indicies,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"num_workers\": self.num_workers,\n",
    "            \"model_state_dict\": self.model.state_dict(),\n",
    "            \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "            \"lr_scheduler_state_dict\": self.lr_scheduler.state_dict(),\n",
    "            \"stats_tracker\": self.stats_tracker,\n",
    "            \"last_epoch\": self.last_epoch, \n",
    "            \"total_epochs\": self.total_epochs,\n",
    "            \"result_path\": self.result_path,\n",
    "            \"last\": self.last, \n",
    "            \"best\": self.best,\n",
    "        }\n",
    "        torch.save(save_state, self.last)\n",
    "        if is_best:\n",
    "            torch.save(save_state, self.best)\n",
    "        \n",
    "    \n",
    "    def load_state(file: str):\n",
    "        \"\"\"\n",
    "        Loads the training state from a saved file.\n",
    "        Args:\n",
    "          dataset: The AleketDataset object.\n",
    "          file: The path to the saved state file.\n",
    "        \"\"\"\n",
    "        \n",
    "        save_state = torch.load(file, weights_only=False)\n",
    "\n",
    "        train_indicies = save_state[\"train_indicies\"]\n",
    "        val_indicies = save_state[\"val_indicies\"]\n",
    "        batch_size = save_state[\"batch_size\"]\n",
    "        num_workers = save_state[\"num_workers\"]\n",
    "        dataset = save_state[\"dataset\"] \n",
    "        \n",
    "        model = get_model(3)\n",
    "        model.load_state_dict(save_state[\"model_state_dict\"])\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        \n",
    "        optimizer = optim.SGD(params)\n",
    "        optimizer.load_state_dict(save_state[\"optimizer_state_dict\"])\n",
    "        \n",
    "        lr_scheduler = optim.lr_scheduler.LinearLR(optimizer)\n",
    "        lr_scheduler.load_state_dict(save_state[\"lr_scheduler_state_dict\"])\n",
    "        \n",
    "        stats_tracker = save_state[\"stats_tracker\"]  \n",
    "        last_epoch = save_state[\"last_epoch\"]\n",
    "        total_epochs = save_state[\"total_epochs\"]\n",
    "        result_path = save_state[\"result_path\"]\n",
    "        \n",
    "        print(f\"Loaded state from {file}\")\n",
    "        return Trainer(dataset,\n",
    "                       train_indicies,\n",
    "                       val_indicies,\n",
    "                       batch_size,\n",
    "                       num_workers,\n",
    "                       model,\n",
    "                       optimizer,\n",
    "                       lr_scheduler,\n",
    "                       result_path,\n",
    "                       total_epochs,\n",
    "                       stats_tracker,\n",
    "                       last_epoch)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G9UdbXiJIAzZ",
    "outputId": "4eaac050-8192-4ffe-e5ae-ee59701ce7b2"
   },
   "outputs": [],
   "source": [
    "# Random Seed for Reproducibility\n",
    "SEED = 1\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Downloads and extracts the dataset if it doesn't exist locally.\n",
    "def download_dataset(save_dir: str):\n",
    "    \"\"\"Downloads and extracts the dataset if it doesn't exist locally.\n",
    "\n",
    "    Args:\n",
    "        save_dir: The directory to save the dataset.\n",
    "\n",
    "    Returns:\n",
    "        The path to the saved dataset directory.\n",
    "    \"\"\"\n",
    "    patched_dataset_gdrive_id = \"\"  #  FIXME: Replace with actual Google Drive ID fpr\n",
    "    if not os.path.exists(save_dir):\n",
    "        gdown.download(id=patched_dataset_gdrive_id, output=\"_temp_.zip\")\n",
    "        shutil.unpack_archive(\"_temp_.zip\", save_dir)\n",
    "        os.remove(\"_temp_.zip\")\n",
    "    print(f\"Dataset loaded from {save_dir}\")\n",
    "    return save_dir\n",
    "\n",
    "\n",
    "# Path variables\n",
    "RESUME = False\n",
    "RESULT_PATH = \"result\"\n",
    "DATASET_ROOT = download_dataset(\"dataset_patched\")\n",
    "\n",
    "# Dataset split\n",
    "TEST_FRACTION = 0.2\n",
    "DATASET_FRACTION = 1\n",
    "DATALOADER_WORKERS = 16\n",
    "\n",
    "# Training Hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 500\n",
    "LR = 0.005\n",
    "MOMENTUM = 0.997\n",
    "WEIGHT_DECAY = 0.0001\n",
    "WARMUP_EPOCHS = 15\n",
    "# Data Augmentation Transforms\n",
    "TRANSFORMS = v2.Compose(\n",
    "    [\n",
    "        v2.RandomHorizontalFlip(p=0.5),\n",
    "        v2.RandomVerticalFlip(p=0.5),\n",
    "        v2.RandomPerspective(distortion_scale=0.2, p=0.2),\n",
    "        v2.RandomRotation(degrees=(-10, 10), expand=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainer = None\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training and evaluation loop.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    if not RESUME:\n",
    "        dataset = AleketDataset(DATASET_ROOT, transforms=TRANSFORMS)\n",
    "        # Calculate the number of samples to use based on the dataset_fraction\n",
    "        num_samples = int(len(dataset) * DATASET_FRACTION)\n",
    "        indices = torch.randperm(len(dataset)).tolist()[:num_samples]\n",
    "        test_size = int(num_samples * TEST_FRACTION)\n",
    "        train_indicies, val_indicies = indices[:-test_size], indices[-test_size:]\n",
    "        \n",
    "        model = get_model(3)\n",
    "        params = [\n",
    "            p for p in model.parameters() if p.requires_grad\n",
    "        ]  # Optimize only trainable parameters\n",
    "        optimizer = optim.SGD(params, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer, start_factor=1, end_factor=0.1, total_iters=WARMUP_EPOCHS\n",
    "        )\n",
    "\n",
    "        stats_tracker = StatsTracker()\n",
    "\n",
    "        trainer = Trainer(\n",
    "            dataset,\n",
    "            train_indicies,\n",
    "            val_indicies,\n",
    "            BATCH_SIZE,\n",
    "            DATALOADER_WORKERS,\n",
    "            model,\n",
    "            optimizer,\n",
    "            lr_scheduler,\n",
    "            RESULT_PATH,\n",
    "            EPOCHS,\n",
    "            stats_tracker,\n",
    "        )\n",
    "    else: \n",
    "        trainer = Trainer.load_state(\"result/run/best.pth\")\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "https://github.com/maximusjv/AleketML/blob/main/aleket.ipynb",
     "timestamp": 1726685419266
    }
   ]
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
