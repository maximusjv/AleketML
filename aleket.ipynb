{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKXbvPtmIAzW"
   },
   "source": [
    "# Aleket Faster R-CNN training notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10801,
     "status": "ok",
     "timestamp": 1726688877827,
     "user": {
      "displayName": "maximus JV",
      "userId": "01037924598235015782"
     },
     "user_tz": -180
    },
    "id": "oLaoTSNaIAzX"
   },
   "outputs": [],
   "source": [
    "%pip install pillow\n",
    "%pip install numpy<2.0\n",
    "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cu124\n",
    "%pip install matplotlib\n",
    "%pip install pycocotools\n",
    "%pip install gdown\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "print(\"ALL DEPENDENCIES INSTALLED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 7480,
     "status": "ok",
     "timestamp": 1726688885304,
     "user": {
      "displayName": "maximus JV",
      "userId": "01037924598235015782"
     },
     "user_tz": -180
    },
    "id": "QYh3of1UgFZs"
   },
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "# Standard Library\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Third-Party Libraries\n",
    "import gdown\n",
    "import numpy as np\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# Torchvision\n",
    "import torchvision.models.detection as tv_detection\n",
    "import torchvision.transforms.v2 as v2\n",
    "\n",
    "# Utils\n",
    "from utils import AleketDataset, TrainingLogger, StatsTracker\n",
    "from training_and_evaluation import evaluate, train_one_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH VARIABLES\n",
    "def download_dataset(save_dir: str):\n",
    "    \"\"\"Downloads and extracts the dataset if it doesn't exist locally.\n",
    "    \n",
    "    Args:\n",
    "        save_dir: The directory to save the dataset.\n",
    "\n",
    "    Returns:\n",
    "        The path to the saved dataset directory.\n",
    "    \"\"\"\n",
    "    patched_dataset_gdrive_id = \"\"  #  FIXME: Replace with your actual Google Drive ID\n",
    "    if not os.path.exists(save_dir):\n",
    "        gdown.download(id=patched_dataset_gdrive_id, output=\"_temp_.zip\")\n",
    "        shutil.unpack_archive(\"_temp_.zip\", save_dir)\n",
    "        os.remove(\"_temp_.zip\")\n",
    "    print(f\"Dataset loaded from {save_dir}\")\n",
    "    return save_dir\n",
    "\n",
    "\n",
    "# Dataset and Model Paths\n",
    "DATASET_ROOT = download_dataset(\"dataset_patched\")\n",
    "MODEL_DIR = \"result\"\n",
    "LAST_MODEL_PATH = os.path.join(MODEL_DIR, \"last_model.pth\")\n",
    "BEST_MODEL_PATH = os.path.join(MODEL_DIR, \"best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN SETTINGS\n",
    "\n",
    "# Device Selection\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Random Seed for Reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Training Hyperparameters\n",
    "LOAD_BEST = False\n",
    "BATCH_SIZE = 15\n",
    "EPOCHS = 500\n",
    "TEST_FRACTION = 0.2\n",
    "DATASET_FRACTION = 1\n",
    "DATALOADER_WORKERS = 20\n",
    "LR = 0.003\n",
    "\n",
    "# Data Augmentation Transforms\n",
    "TRANSFORMS = v2.Compose([\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.RandomVerticalFlip(p=0.5),\n",
    "    v2.RandomPerspective(distortion_scale=0.2, p=0.2),\n",
    "    v2.RandomRotation(degrees=(-10, 10), expand=True),\n",
    "])\n",
    "\n",
    "\n",
    "def get_model(num_classes) -> tv_detection.FasterRCNN:\n",
    "    \"\"\"Loads or creates a Faster R-CNN model.\n",
    "    Args:\n",
    "        num_classes: The number of classes in the dataset.\n",
    "\n",
    "    Returns:\n",
    "        The Faster R-CNN model on the specified device.\n",
    "    \"\"\"\n",
    "    if LOAD_BEST and os.path.exists(BEST_MODEL_PATH):\n",
    "        model = torch.load(BEST_MODEL_PATH, weights_only=False)\n",
    "    else:\n",
    "        model = tv_detection.fasterrcnn_resnet50_fpn_v2(\n",
    "            weights=\"DEFAULT\"\n",
    "        )\n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "        model.roi_heads.box_predictor = (\n",
    "            tv_detection.faster_rcnn.FastRCNNPredictor(\n",
    "                in_features, num_classes\n",
    "            )\n",
    "        )\n",
    "    return model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G9UdbXiJIAzZ",
    "outputId": "4eaac050-8192-4ffe-e5ae-ee59701ce7b2"
   },
   "outputs": [],
   "source": [
    "# main\n",
    "\n",
    "def set_seed():\n",
    "    \"\"\"Sets the random seed for reproducibility.\"\"\"\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "\n",
    "def save_model(model, is_best):\n",
    "    \"\"\"Saves the model checkpoint.\n",
    "    Args:\n",
    "        model: The model to save.\n",
    "        is_best: Whether this is the best model so far.\n",
    "    \"\"\"\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    torch.save(model, LAST_MODEL_PATH)\n",
    "    if is_best:\n",
    "        torch.save(model, BEST_MODEL_PATH)\n",
    "\n",
    "\n",
    "def divide_dataset(\n",
    "    dataset: AleketDataset,\n",
    "    dataset_fraction: float,\n",
    "    test_fraction: float,\n",
    "    batch_size: int,\n",
    "    num_workers: int,\n",
    ") -> tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"Divides the dataset into training and validation sets and creates DataLoaders.\n",
    "    Args:\n",
    "        dataset: The AleketDataset to divide.\n",
    "        dataset_fraction: The fraction of the dataset to use.\n",
    "        test_fraction: The fraction of the used dataset to allocate for validation.\n",
    "        batch_size: The batch size for the DataLoaders.\n",
    "        num_workers: The number of worker processes for data loading.\n",
    "    Returns:\n",
    "        A tuple containing the training DataLoader and the validation DataLoader.\n",
    "    \"\"\"\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        \"\"\"Collates data samples into batches for the dataloader.\"\"\"\n",
    "        return tuple(zip(*batch))\n",
    "\n",
    "    # Calculate the number of samples to use based on the dataset_fraction\n",
    "    num_samples = int(len(dataset) * dataset_fraction)\n",
    "\n",
    "    # Randomly shuffle indices and select the desired number of samples\n",
    "    indices = torch.randperm(len(dataset)).tolist()[:num_samples]\n",
    "\n",
    "    # Calculate the number of samples for the validation set\n",
    "    test_size = int(num_samples * test_fraction)\n",
    "\n",
    "    # Create training and validation subsets\n",
    "    train_dataset = Subset(dataset, indices[:-test_size])\n",
    "    val_dataset = Subset(dataset, indices[-test_size:])\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training and evaluation loop.\"\"\"\n",
    "    set_seed()\n",
    "    model = get_model(3)\n",
    "\n",
    "    dataset = AleketDataset(DATASET_ROOT, transforms=TRANSFORMS)\n",
    "    train_dataloader, val_dataloader = divide_dataset(\n",
    "        dataset, DATASET_FRACTION, TEST_FRACTION, BATCH_SIZE, DATALOADER_WORKERS\n",
    "    )\n",
    "\n",
    "    params = [\n",
    "        p for p in model.parameters() if p.requires_grad\n",
    "    ]  # Optimize only trainable parameters\n",
    "    optimizer = optim.SGD(params, lr=LR, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer, start_factor=1, end_factor=0.1, total_iters=15\n",
    "    )\n",
    "\n",
    "    if(os.path.exists(\"training.log\")):\n",
    "        os.remove(\"training.log\")\n",
    "        \n",
    "    logger = TrainingLogger(\"resnet50_v2_backbone training\", \"training.log\", batch_print=False)\n",
    "    stats = StatsTracker()\n",
    "    conf_thresh = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        logger.log_epoch_start(epoch, EPOCHS)\n",
    "        \n",
    "        dataset.train = True\n",
    "        losses = train_one_epoch(\n",
    "            model, optimizer, train_dataloader, DEVICE, epoch, logger\n",
    "        )\n",
    "\n",
    "        dataset.train = False\n",
    "        eval_stats = evaluate(\n",
    "            model, val_dataloader, DEVICE, logger, conf_thresh\n",
    "        )\n",
    "        stats.update_train_loss(losses)\n",
    "        is_best = stats.update_val_metrics(eval_stats)\n",
    "\n",
    "        logger.log_epoch_end(epoch, losses['loss'],eval_stats)\n",
    "        save_model(model, is_best)\n",
    "\n",
    "        lr_scheduler.step()\n",
    "        stats.plot_stats(\"training_stats\")\n",
    "\n",
    "    stats.plot_stats(\"training_stats\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "https://github.com/maximusjv/AleketML/blob/main/aleket.ipynb",
     "timestamp": 1726685419266
    }
   ]
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
